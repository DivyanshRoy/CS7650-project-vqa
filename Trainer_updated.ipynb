{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trainer_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0-zsbXr35M9",
        "colab_type": "code",
        "outputId": "bac5f0be-3097-4a91-e16a-915daf70febc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY0BFQ0x3_zj",
        "colab_type": "code",
        "outputId": "2864e0fe-ce71-41e6-d0e0-a7f96cdd5331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd \"drive/My Drive/VQA-master\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/VQA-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaLLVY4u1t5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1PHaehF3B3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_image_features=True\n",
        "batch_size = 1000\n",
        "print_every = 10\n",
        "\n",
        "input_options = {1: \"resnet 18 image features, question\", 2: \"resnet 192 image features, question\", 3: \"new resnet 18 features\", 4: \"new resnet 152 features\"}\n",
        "model_input = input_options[3]\n",
        "\n",
        "answer_type_options = {1: \"yesno\", 2: \"number\", 3: \"other\"}\n",
        "answer_type = answer_type_options[3]\n",
        "\n",
        "if answer_type == \"yesno\":\n",
        "  lstm_hidden_size = 64\n",
        "  lstm_num_layers = 1  \n",
        "else:\n",
        "  lstm_hidden_size = 256\n",
        "  lstm_num_layers = 1\n",
        "\n",
        "fusion_type_options = {1: \"concatenation\", 2: \"pointwise_mul\", 3: \"try\"}\n",
        "fusion_type = fusion_type_options[3]\n",
        "\n",
        "model_type_options = {1: \"simplecnnlstm\", 2: \"fusion\", 3: \"transformer\"}\n",
        "model_type = model_type_options[3]\n",
        "\n",
        "#works only for fusion model\n",
        "dropout_amount = 0.05\n",
        "\n",
        "Top100 = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUEHiTQc372N",
        "colab_type": "text"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ8YZdq73Emg",
        "colab_type": "code",
        "outputId": "535fcd56-49cc-4751-b24d-7c3315e4c853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "new_mapping = {}\n",
        "train_questions = np.load(\"train_questions.npy\")\n",
        "val_questions = np.load(\"val_questions.npy\")\n",
        "cnt = 0\n",
        "for i in range(train_questions.shape[0]):\n",
        "  for j in range(train_questions.shape[1]):\n",
        "    v = train_questions[i][j]\n",
        "    if v not in new_mapping:\n",
        "      new_mapping[v] = cnt\n",
        "      cnt += 1\n",
        "for i in range(val_questions.shape[0]):\n",
        "  for j in range(val_questions.shape[1]):\n",
        "    v = val_questions[i][j]\n",
        "    if v not in new_mapping:\n",
        "      new_mapping[v] = cnt\n",
        "      cnt += 1\n",
        "print(cnt)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC1FkQQ8MMkM",
        "colab_type": "code",
        "outputId": "4d9e40d4-917e-43ea-b5bd-6df424b08564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "new_mapping2 = {}\n",
        "train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "cnt2 = 0\n",
        "for i in range(train_questions.shape[0]):\n",
        "  for j in range(train_questions.shape[1]):\n",
        "    v = train_questions[i][j]\n",
        "    if v not in new_mapping2:\n",
        "      new_mapping2[v] = cnt2\n",
        "      cnt2 += 1\n",
        "for i in range(val_questions.shape[0]):\n",
        "  for j in range(val_questions.shape[1]):\n",
        "    v = val_questions[i][j]\n",
        "    if v not in new_mapping2:\n",
        "      new_mapping2[v] = cnt2\n",
        "      cnt2 += 1\n",
        "print(cnt2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjb84H5dgap7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "if model_input == \"resnet 18 image features, question\":\n",
        "  # train_answers = np.load(\"train_answers_\"+answer_type+\".npy\")\n",
        "  if Top100 == True:\n",
        "    train_indices = np.load(\"top100_train_indices.npy\")\n",
        "    val_indices = np.load(\"top100_val_indices.npy\")\n",
        "    \n",
        "  train_answers = np.load(\"train_answers.npy\")\n",
        "  if Top100 == True:\n",
        "    \n",
        "    train_answers = train_answers[train_indices]\n",
        "    answer_dict = {}\n",
        "    cnt = 0\n",
        "    print(train_answers.shape)\n",
        "    for i in range(train_answers.shape[0]):\n",
        "      v = train_answers[i][0]\n",
        "      if v not in answer_dict:\n",
        "        answer_dict[v] = cnt\n",
        "        cnt += 1\n",
        "    print(cnt)\n",
        "\n",
        "    print(answer_dict)\n",
        "    for i in range(train_answers.shape[0]):\n",
        "      train_answers[i,0] = answer_dict[train_answers[i,0]]\n",
        "    # train_answers = answer_dict[train_answers[:,0]]\n",
        "    # print(train_answers.shape)\n",
        "\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  # val_answers = np.load(\"val_answers_\"+answer_type+\".npy\")\n",
        "  val_answers = np.load(\"val_answers.npy\")\n",
        "  if Top100 == True:\n",
        "    val_answers = val_answers[val_indices]\n",
        "    for i in range(val_answers.shape[0]):\n",
        "      val_answers[i,0] = answer_dict[val_answers[i,0]]\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  \n",
        "elif model_input == \"resnet 192 image features, question\":\n",
        "  train_answers = np.load(\"train_answers_\"+answer_type+\".npy\")\n",
        "  # train_answers = np.load(\"train_answers.npy\")\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  val_answers = np.load(\"val_answers_\"+answer_type+\".npy\")\n",
        "  # val_answers = np.load(\"val_answers.npy\")\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  # train_indices = np.load(\"top100_train_indices.npy\")\n",
        "  # val_indices = np.load(\"top100_val_indices.npy\")\n",
        "elif model_input == \"new resnet 18 features\":\n",
        "  train_answers = np.load(\"filtered_train_answers.npy\")\n",
        "  # train_answers = np.load(\"train_answers.npy\")\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  val_answers = np.load(\"filtered_val_answers.npy\")\n",
        "  # val_answers = np.load(\"val_answers.npy\")\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  # train_indices = np.load(\"top100_train_indices.npy\")\n",
        "  # val_indices = np.load(\"top100_val_indices.npy\")\n",
        "\n",
        "\n",
        "if model_input == \"resnet 18 image features, question\":\n",
        "  # train_images = np.load(\"train_image_features_\"+answer_type+\".npy\")\n",
        "  # train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  # val_images = np.load(\"val_image_features_\"+answer_type+\".npy\")\n",
        "  # val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  train_images = np.load(\"train_image_features.npy\")\n",
        "  train_questions = np.load(\"train_questions.npy\")\n",
        "  for i in range(train_questions.shape[0]):\n",
        "    for j in range(train_questions.shape[1]):\n",
        "      v = train_questions[i,j]\n",
        "      train_questions[i,j] = new_mapping[v]\n",
        "  # train_questions = train_questions[:,:20]\n",
        "\n",
        "  if Top100 == True:\n",
        "    train_images = train_images[train_indices]\n",
        "    train_questions = train_questions[train_indices]\n",
        "\n",
        "  val_images = np.load(\"val_image_features.npy\")\n",
        "  val_questions = np.load(\"val_questions.npy\")\n",
        "  for i in range(val_questions.shape[0]):\n",
        "    for j in range(val_questions.shape[1]):\n",
        "      v = val_questions[i,j]\n",
        "      val_questions[i,j] = new_mapping[v]\n",
        "\n",
        "  # val_questions = val_questions[:,:20]\n",
        "\n",
        "  if Top100 == True:\n",
        "    val_images = val_images[val_indices]\n",
        "    val_questions = val_questions[val_indices]\n",
        "\n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "elif model_input == \"resnet 192 image features, question\":\n",
        "  train_images = np.load(\"train_image_features_\"+answer_type+\".npy\")\n",
        "  train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  val_images = np.load(\"val_image_features_\"+answer_type+\".npy\")\n",
        "  val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "  \n",
        "  for i in range(train_questions.shape[0]):\n",
        "    for j in range(train_questions.shape[1]):\n",
        "      v = train_questions[i,j]\n",
        "      train_questions[i,j] = new_mapping2[v]\n",
        "\n",
        "\n",
        "  for i in range(val_questions.shape[0]):\n",
        "    for j in range(val_questions.shape[1]):\n",
        "      v = val_questions[i,j]\n",
        "      val_questions[i,j] = new_mapping2[v]\n",
        "\n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "elif model_input == \"new resnet 18 features\":\n",
        "  train_images = np.load(\"filtered_train_image_features_18.npy\")\n",
        "  train_questions = np.load(\"filtered_train_questions.npy\")\n",
        "\n",
        "  val_images = np.load(\"filtered_val_image_features_18.npy\")\n",
        "  val_questions = np.load(\"filtered_val_questions.npy\")\n",
        "  \n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4af-oe4XJlsy",
        "colab_type": "code",
        "outputId": "fd14554e-3f10-46a1-d14a-b160d28a054d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_images.shape)\n",
        "print(val_images.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13000, 512)\n",
            "(2600, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7JuABKSJqSw",
        "colab_type": "code",
        "outputId": "7a674be6-effd-4c72-9d87-c7888b8fb909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_questions.shape)\n",
        "print(val_questions.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13000, 25)\n",
            "(2600, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mif2irg9KbaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57Wbz-44HBd",
        "colab_type": "text"
      },
      "source": [
        "#### Choose model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZFIWKQnz7om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_type == \"simplecnnlstm\":\n",
        "  import models.Simple_CNN_LSTM_Model as models\n",
        "  # import models.trial_model as trial_model\n",
        "elif model_type == \"fusion\":\n",
        "  import models.fusion_model as fusion_model\n",
        "elif model_type == \"transformer\":\n",
        "  import models.transformer_model as transformer_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvpONlFiTMZk",
        "colab_type": "code",
        "outputId": "86049ca3-0564-4baf-bd56-7753746c6542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(lstm_hidden_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynecbLrqOxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_matrix = np.load(\"weights_matrix.npy\")\n",
        "weights_matrix = torch.tensor(weights_matrix)\n",
        "if model_type == \"simplecnnlstm\":\n",
        "  model = models.SimpleCNNLSTM(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, Top100=Top100)\n",
        "elif model_type == \"fusion\":\n",
        "  if answer_type == \"yesno\":\n",
        "    model = fusion_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=32, dropout_amount=dropout_amount, Top100=Top100)\n",
        "  else:\n",
        "    model = fusion_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=256, dropout_amount=dropout_amount, Top100=Top100)\n",
        "elif model_type == \"transformer\":\n",
        "  if answer_type == \"yesno\":\n",
        "    model = transformer_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=32, dropout_amount=dropout_amount, Top100=Top100)\n",
        "  else:\n",
        "    model = transformer_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=256, dropout_amount=dropout_amount, Top100=Top100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bU4rv0_V12s",
        "colab_type": "code",
        "outputId": "13325302-2bb3-4def-ffab-8169011e6461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FusionModel(\n",
            "  (transformer_model): TransformerModel(\n",
            "    (pos_encoder): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.05, inplace=False)\n",
            "    )\n",
            "    (transformer_encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.05, inplace=False)\n",
            "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.05, inplace=False)\n",
            "          (dropout2): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (encoder): Embedding(3100, 512)\n",
            "    (decoder): Linear(in_features=512, out_features=3100, bias=True)\n",
            "  )\n",
            "  (img_dense1): Linear(in_features=512, out_features=12800, bias=True)\n",
            "  (img_dense2): Linear(in_features=12800, out_features=256, bias=True)\n",
            "  (q_dense1): Linear(in_features=12800, out_features=512, bias=True)\n",
            "  (q_dense2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc1): Linear(in_features=256, out_features=13, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (dropout): Dropout(p=0.05, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dS4E_c-4dp_",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_czxJkrlwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "lr_reducer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=np.sqrt(0.1),\n",
        "                                                                        cooldown=0,\n",
        "                                                                        patience=3,\n",
        "                                                                        min_lr=0.5e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUsFk8z_2XJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(predictions, labels):\n",
        "  predictions = torch.max(predictions, axis=1)[1]\n",
        "  ab = torch.abs(predictions-labels)\n",
        "  ab = ab.detach().numpy()\n",
        "  mn = np.minimum(ab, 1)\n",
        "  eq = 1-mn\n",
        "  correct = np.sum(eq)\n",
        "  total = eq.shape[0]\n",
        "  return correct, total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUmF088VgUuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(y_pred, y_true, gamma=2., alpha=4.):\n",
        "\n",
        "    gamma = float(gamma)\n",
        "    alpha = float(alpha)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Focal loss for multi-classification\n",
        "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
        "        Notice: y_pred is probability after softmax\n",
        "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
        "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
        "        Focal Loss for Dense Object Detection\n",
        "        https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Arguments:\n",
        "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
        "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
        "\n",
        "        Keyword Arguments:\n",
        "            gamma {float} -- (default: {2.0})\n",
        "            alpha {float} -- (default: {4.0})\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- loss.\n",
        "        \"\"\"\n",
        "        # y_pred = y_pred.detach().numpy()\n",
        "        # y_pred = np.array(y_pred, dtype=np.int32)\n",
        "        # print(y_true.shape)\n",
        "        # print(y_pred.shape)\n",
        "        # y_pred_onehot = np.zeros(shape=[y_pred.shape[0], y_true.size(1)])\n",
        "        # y_pred_onehot[:,y_pred] = 1\n",
        "        # if answer_type != \"yesno\":\n",
        "        y_true_onehot = torch.FloatTensor(y_pred.size())\n",
        "        y_true_onehot.zero_()\n",
        "        y_true_onehot.scatter_(1, y_true.type(torch.long).view(y_true.size(0),1), 1.0)\n",
        "        y_true = y_true_onehot\n",
        "\n",
        "        epsilon = 1.e-9\n",
        "        # y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
        "        # y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
        "        y_pred = y_pred.type(torch.float32)\n",
        "        y_true = y_true.type(torch.float32)\n",
        "\n",
        "        # print(torch.max(y_true[:10], axis=1))\n",
        "        # print(torch.max(y_pred[:10], axis=1))\n",
        "\n",
        "        model_out = torch.add(y_pred, epsilon)\n",
        "        # print(\"model_out: \",model_out[:1])\n",
        "        # model_out = tf.add(y_pred, epsilon)\n",
        "        ce = torch.mul(y_true, torch.log(model_out))\n",
        "        # print(\"ce: \",ce[:1])\n",
        "        # ce = tf.multiply(y_true, -tf.log(model_out))\n",
        "        # print(\"1-model_out: \",(1-model_out)[:1])\n",
        "        # print(\"gamma: \",gamma)\n",
        "        # print(\"pow: \",torch.pow((1-model_out), gamma)[:1])\n",
        "        weight = torch.mul(y_true, torch.pow((1-model_out), gamma))\n",
        "        # print(\"weight: \",weight[:1])\n",
        "        # weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
        "        fl = -1*torch.mul(torch.mul(alpha, weight), ce)\n",
        "        # print(\"fl: \",fl[:1])\n",
        "        # fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
        "        reduced_fl = torch.max(fl, axis=1)[0]\n",
        "        # print(\"reduced_fl: \",reduced_fl[:1])\n",
        "        # reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return torch.mean(reduced_fl)\n",
        "        # return tf.reduce_mean(reduced_fl)\n",
        "    loss = focal_loss_fixed(y_true, y_pred)\n",
        "    # print(loss)\n",
        "    return loss\n",
        "\n",
        "alpha = 4.\n",
        "gamma = 2."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcEf05wAmYap",
        "colab_type": "code",
        "outputId": "acc27106-302c-4326-9244-1c8ce1f6968b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        }
      },
      "source": [
        "for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    num_batches = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "    # for i, data in enumerate(valloader, 0):\n",
        "        # if i % print_every == 0 and i>0:\n",
        "          # print(\"Train batch \",i+1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\":\n",
        "          image_features, questions, labels = data\n",
        "          # print(labels.size())\n",
        "          image_features = image_features.type(torch.float32)\n",
        "          labels = labels.flatten().type(torch.long)\n",
        "          labels = labels.view(-1)\n",
        "\n",
        "          outputs = model(image_features, questions)\n",
        "          # print(\"out done\")\n",
        "\n",
        "        \n",
        "        batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "        # print(\"accuracy done\")\n",
        "        correct += batch_correct\n",
        "        total += batch_total\n",
        "        # loss = focal_loss(outputs, labels, alpha=alpha, gamma=gamma)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # print(\"loss done\")\n",
        "        loss.backward()\n",
        "        # print(\"backprop done\")\n",
        "        optimizer.step()\n",
        "        # print(\"optim done\")\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_loss += running_loss\n",
        "        num_batches += 1\n",
        "        running_loss = 0.0\n",
        "\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_batches = 0\n",
        "    val_total_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      # for i, data in enumerate(trainloader, 0):\n",
        "      for i, data in enumerate(valloader, 0):\n",
        "          # if i % print_every == 0 and i>0:\n",
        "            # print(\"Val batch \",i+1)\n",
        "\n",
        "          if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\":\n",
        "            image_features, questions, labels = data\n",
        "            image_features = image_features.type(torch.float32)\n",
        "            labels = labels.flatten().type(torch.long)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            outputs = model(image_features, questions)\n",
        "\n",
        "          batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "          val_correct += batch_correct\n",
        "          val_total += batch_total\n",
        "          val_batches += 1\n",
        "\n",
        "          # val_loss = focal_loss(outputs, labels, alpha=alpha, gamma=gamma)\n",
        "          val_loss = criterion(outputs, labels)\n",
        "          val_total_loss += val_loss.detach().numpy()\n",
        "          lr_reducer.step(val_loss)\n",
        "\n",
        "          \n",
        "    print(\"Epoch: \",epoch+1,\" Loss: \",total_loss/num_batches,\" Train-Accuracy: \", correct/total,\" Val Loss: \",val_total_loss/val_batches,\" Val-Accuracy: \",val_correct/val_total)\n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1  Loss:  2.478324321600107  Train-Accuracy:  0.20553846153846153  Val Loss:  2.494877338409424  Val-Accuracy:  0.26153846153846155\n",
            "Epoch:  2  Loss:  2.3846846177027774  Train-Accuracy:  0.2980769230769231  Val Loss:  2.459228833516439  Val-Accuracy:  0.2773076923076923\n",
            "Epoch:  3  Loss:  2.3464840742257924  Train-Accuracy:  0.3396923076923077  Val Loss:  2.456593672434489  Val-Accuracy:  0.26346153846153847\n",
            "Epoch:  4  Loss:  2.3321154484382043  Train-Accuracy:  0.35107692307692306  Val Loss:  2.4404305617014566  Val-Accuracy:  0.2796153846153846\n",
            "Epoch:  5  Loss:  2.298803604566134  Train-Accuracy:  0.3859230769230769  Val Loss:  2.4459386666615806  Val-Accuracy:  0.29115384615384615\n",
            "Epoch:  6  Loss:  2.2647545337677  Train-Accuracy:  0.42315384615384616  Val Loss:  2.439512252807617  Val-Accuracy:  0.30153846153846153\n",
            "Epoch:  7  Loss:  2.239628003193782  Train-Accuracy:  0.44969230769230767  Val Loss:  2.4371068477630615  Val-Accuracy:  0.3\n",
            "Epoch:  8  Loss:  2.218923642085149  Train-Accuracy:  0.4727692307692308  Val Loss:  2.437835137049357  Val-Accuracy:  0.30615384615384617\n",
            "Epoch:  9  Loss:  2.2091111953441915  Train-Accuracy:  0.4866923076923077  Val Loss:  2.4379576047261557  Val-Accuracy:  0.2976923076923077\n",
            "Epoch:  10  Loss:  2.206573724746704  Train-Accuracy:  0.4860769230769231  Val Loss:  2.4413835207621255  Val-Accuracy:  0.29730769230769233\n",
            "Epoch:  11  Loss:  2.2051920157212477  Train-Accuracy:  0.488  Val Loss:  2.441664218902588  Val-Accuracy:  0.3030769230769231\n",
            "Epoch:  12  Loss:  2.204167989584116  Train-Accuracy:  0.4886153846153846  Val Loss:  2.4412089188893638  Val-Accuracy:  0.3038461538461538\n",
            "Epoch:  13  Loss:  2.2023123777829685  Train-Accuracy:  0.49253846153846154  Val Loss:  2.438764810562134  Val-Accuracy:  0.29730769230769233\n",
            "Epoch:  14  Loss:  2.2029174841367283  Train-Accuracy:  0.49046153846153845  Val Loss:  2.4430739879608154  Val-Accuracy:  0.2919230769230769\n",
            "Epoch:  15  Loss:  2.2059423189896803  Train-Accuracy:  0.48746153846153845  Val Loss:  2.4397056102752686  Val-Accuracy:  0.29423076923076924\n",
            "Epoch:  16  Loss:  2.2039419137514553  Train-Accuracy:  0.48946153846153845  Val Loss:  2.439370552698771  Val-Accuracy:  0.30423076923076925\n",
            "Epoch:  17  Loss:  2.201145740655752  Train-Accuracy:  0.49446153846153845  Val Loss:  2.441687742869059  Val-Accuracy:  0.30346153846153845\n",
            "Epoch:  18  Loss:  2.200215926537147  Train-Accuracy:  0.493  Val Loss:  2.439891974131266  Val-Accuracy:  0.3038461538461538\n",
            "Epoch:  19  Loss:  2.202429716403668  Train-Accuracy:  0.49146153846153845  Val Loss:  2.437907059987386  Val-Accuracy:  0.31076923076923074\n",
            "Epoch:  20  Loss:  2.2038880861722507  Train-Accuracy:  0.4906923076923077  Val Loss:  2.4417441685994468  Val-Accuracy:  0.3073076923076923\n",
            "Epoch:  21  Loss:  2.204057271663959  Train-Accuracy:  0.49053846153846153  Val Loss:  2.4382776419321694  Val-Accuracy:  0.31153846153846154\n",
            "Epoch:  22  Loss:  2.2021623391371508  Train-Accuracy:  0.4926923076923077  Val Loss:  2.440735419591268  Val-Accuracy:  0.3069230769230769\n",
            "Epoch:  23  Loss:  2.201509218949538  Train-Accuracy:  0.4933076923076923  Val Loss:  2.4400230248769126  Val-Accuracy:  0.2969230769230769\n",
            "Epoch:  24  Loss:  2.201948514351478  Train-Accuracy:  0.4939230769230769  Val Loss:  2.440175771713257  Val-Accuracy:  0.31038461538461537\n",
            "Epoch:  25  Loss:  2.203278523225051  Train-Accuracy:  0.49038461538461536  Val Loss:  2.438674529393514  Val-Accuracy:  0.30423076923076925\n",
            "Epoch:  26  Loss:  2.2024415089533877  Train-Accuracy:  0.4919230769230769  Val Loss:  2.4410152435302734  Val-Accuracy:  0.3038461538461538\n",
            "Epoch:  27  Loss:  2.204579921869131  Train-Accuracy:  0.4886923076923077  Val Loss:  2.4400903383890786  Val-Accuracy:  0.2980769230769231\n",
            "Epoch:  28  Loss:  2.2028952011695275  Train-Accuracy:  0.49115384615384616  Val Loss:  2.439006487528483  Val-Accuracy:  0.29615384615384616\n",
            "Epoch:  29  Loss:  2.2021680795229397  Train-Accuracy:  0.49215384615384616  Val Loss:  2.440213123957316  Val-Accuracy:  0.29923076923076924\n",
            "Epoch:  30  Loss:  2.2034382086533766  Train-Accuracy:  0.4898461538461538  Val Loss:  2.440329392751058  Val-Accuracy:  0.3019230769230769\n",
            "Epoch:  31  Loss:  2.2035109446598935  Train-Accuracy:  0.488  Val Loss:  2.438730478286743  Val-Accuracy:  0.30346153846153845\n",
            "Epoch:  32  Loss:  2.2020743993612437  Train-Accuracy:  0.49  Val Loss:  2.4417545795440674  Val-Accuracy:  0.3046153846153846\n",
            "Epoch:  33  Loss:  2.2025018471937914  Train-Accuracy:  0.49015384615384616  Val Loss:  2.4410187403361  Val-Accuracy:  0.2988461538461539\n",
            "Epoch:  34  Loss:  2.202071134860699  Train-Accuracy:  0.49338461538461537  Val Loss:  2.4413986206054688  Val-Accuracy:  0.3019230769230769\n",
            "Epoch:  35  Loss:  2.2032730395977316  Train-Accuracy:  0.49030769230769233  Val Loss:  2.4376488526662192  Val-Accuracy:  0.30538461538461537\n",
            "Epoch:  36  Loss:  2.2029169706197886  Train-Accuracy:  0.49130769230769233  Val Loss:  2.441939910252889  Val-Accuracy:  0.30115384615384616\n",
            "Epoch:  37  Loss:  2.203517345281748  Train-Accuracy:  0.48953846153846153  Val Loss:  2.44087553024292  Val-Accuracy:  0.2969230769230769\n",
            "Epoch:  38  Loss:  2.201477894416222  Train-Accuracy:  0.4930769230769231  Val Loss:  2.440486749013265  Val-Accuracy:  0.30038461538461536\n",
            "Epoch:  39  Loss:  2.2037184972029467  Train-Accuracy:  0.48830769230769233  Val Loss:  2.438981374104818  Val-Accuracy:  0.3076923076923077\n",
            "Epoch:  40  Loss:  2.2029484051924486  Train-Accuracy:  0.48946153846153845  Val Loss:  2.440505584081014  Val-Accuracy:  0.29307692307692307\n",
            "Epoch:  41  Loss:  2.203403252821702  Train-Accuracy:  0.4906153846153846  Val Loss:  2.441041628519694  Val-Accuracy:  0.30346153846153845\n",
            "Epoch:  42  Loss:  2.2039535412421594  Train-Accuracy:  0.4900769230769231  Val Loss:  2.443442185719808  Val-Accuracy:  0.2976923076923077\n",
            "Epoch:  43  Loss:  2.2018072238335242  Train-Accuracy:  0.4919230769230769  Val Loss:  2.440504709879557  Val-Accuracy:  0.3019230769230769\n",
            "Epoch:  44  Loss:  2.202307077554556  Train-Accuracy:  0.4906153846153846  Val Loss:  2.440141042073568  Val-Accuracy:  0.30423076923076925\n",
            "Epoch:  45  Loss:  2.2041022777557373  Train-Accuracy:  0.49015384615384616  Val Loss:  2.43760347366333  Val-Accuracy:  0.3073076923076923\n",
            "Epoch:  46  Loss:  2.2034842050992527  Train-Accuracy:  0.4906923076923077  Val Loss:  2.4396536350250244  Val-Accuracy:  0.2953846153846154\n",
            "Epoch:  47  Loss:  2.202486955202543  Train-Accuracy:  0.49123076923076925  Val Loss:  2.4406251907348633  Val-Accuracy:  0.2996153846153846\n",
            "Epoch:  48  Loss:  2.2041854491600623  Train-Accuracy:  0.4900769230769231  Val Loss:  2.440481503804525  Val-Accuracy:  0.30538461538461537\n",
            "Epoch:  49  Loss:  2.2031095578120303  Train-Accuracy:  0.49130769230769233  Val Loss:  2.4392286936442056  Val-Accuracy:  0.3038461538461538\n",
            "Epoch:  50  Loss:  2.2024745757763204  Train-Accuracy:  0.49246153846153845  Val Loss:  2.440591732660929  Val-Accuracy:  0.30423076923076925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCd9IxVU3yD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt_dict = {}\n",
        "\n",
        "for i in range(len(train_answers)):\n",
        "  v = train_answers[i][0]\n",
        "  if v not in cnt_dict:\n",
        "    cnt_dict[v] = 1\n",
        "  else:\n",
        "    cnt_dict[v] = cnt_dict[v] + 1\n",
        "print(cnt_dict)\n",
        "\n",
        "cnt_list = []\n",
        "for key in cnt_dict.keys():\n",
        "  v = cnt_dict[key]\n",
        "  if v > 50:\n",
        "    cnt_list.append((key,v))\n",
        "\n",
        "print(len(cnt_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jXAs9MaAAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}