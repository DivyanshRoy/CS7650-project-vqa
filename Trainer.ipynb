{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trainer_(2)_(2)_(3)_(2)_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaLLVY4u1t5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5olTlpVimn_",
        "colab_type": "code",
        "outputId": "1f008143-e4da-44fd-8e97-9368e716d742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.current_device()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ROWpwKinqk",
        "colab_type": "code",
        "outputId": "cb835b4f-3b83-4628-c02b-761e212680c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.device(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.device at 0x7f4f9ae46390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhE1-f2fiqmf",
        "colab_type": "code",
        "outputId": "437fb2e2-48ef-49b1-822f-78cbde7c1b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtrGIuYIitm7",
        "colab_type": "code",
        "outputId": "6185d4af-f56c-40cb-9827-f6fb4b38e3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64HRM4soi_QI",
        "colab_type": "code",
        "outputId": "e2aa4d46-0c6f-453f-df42-ea9e426ed5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1PHaehF3B3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use_image_features=True\n",
        "# batch_size = 1000\n",
        "# print_every = 10\n",
        "\n",
        "# input_options = {1: \"resnet 18 image features, question\", 2: \"resnet 192 image features, question\", 3: \"new resnet 18 features\", 4: \"new resnet 152 features\"}\n",
        "# model_input = input_options[3]\n",
        "\n",
        "# answer_type_options = {1: \"yesno\", 2: \"number\", 3: \"other\"}\n",
        "# answer_type = answer_type_options[3]\n",
        "\n",
        "# if answer_type == \"yesno\":\n",
        "#   lstm_hidden_size = 64\n",
        "#   lstm_num_layers = 1  \n",
        "# else:\n",
        "#   lstm_hidden_size = 256\n",
        "#   lstm_num_layers = 1\n",
        "\n",
        "# fusion_type_options = {1: \"concatenation\", 2: \"pointwise_mul\", 3: \"try\"}\n",
        "# fusion_type = fusion_type_options[1]\n",
        "\n",
        "# model_type_options = {1: \"simplecnnlstm\", 2: \"fusion\", 3: \"transformer\"}\n",
        "# model_type = model_type_options[3]\n",
        "\n",
        "# #works only for fusion model\n",
        "# dropout_amount = 0.01\n",
        "\n",
        "# Top100 = False\n",
        "\n",
        "use_image_features=True\n",
        "batch_size = 1000\n",
        "print_every = 10\n",
        "\n",
        "input_options = {1: \"resnet 18 image features, question\", 2: \"resnet 192 image features, question\", 3: \"new resnet 18 features\", 4: \"new resnet 152 features\"}\n",
        "model_input = input_options[3]\n",
        "\n",
        "answer_type_options = {1: \"yesno\", 2: \"number\", 3: \"other\"}\n",
        "answer_type = answer_type_options[3]\n",
        "\n",
        "if answer_type == \"yesno\":\n",
        "  lstm_hidden_size = 64\n",
        "  lstm_num_layers = 1  \n",
        "else:\n",
        "  lstm_hidden_size = 64\n",
        "  lstm_num_layers = 1\n",
        "\n",
        "fusion_type_options = {1: \"concatenation\", 2: \"pointwise_mul\", 3: \"try\"}\n",
        "fusion_type = fusion_type_options[3]\n",
        "\n",
        "model_type_options = {1: \"simplecnnlstm\", 2: \"fusion\", 3: \"transformer\"}\n",
        "model_type = model_type_options[2]\n",
        "\n",
        "#works only for fusion model\n",
        "dropout_amount = 0.3\n",
        "\n",
        "shared_size = 128\n",
        "\n",
        "Top100 = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUEHiTQc372N",
        "colab_type": "text"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ8YZdq73Emg",
        "colab_type": "code",
        "outputId": "860e659a-188f-4630-eae4-25fd15944892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "new_mapping = {}\n",
        "train_questions = np.load(\"train_questions.npy\")\n",
        "val_questions = np.load(\"val_questions.npy\")\n",
        "cnt = 0\n",
        "for i in range(train_questions.shape[0]):\n",
        "  for j in range(train_questions.shape[1]):\n",
        "    v = train_questions[i][j]\n",
        "    if v not in new_mapping:\n",
        "      new_mapping[v] = cnt\n",
        "      cnt += 1\n",
        "for i in range(val_questions.shape[0]):\n",
        "  for j in range(val_questions.shape[1]):\n",
        "    v = val_questions[i][j]\n",
        "    if v not in new_mapping:\n",
        "      new_mapping[v] = cnt\n",
        "      cnt += 1\n",
        "print(cnt)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC1FkQQ8MMkM",
        "colab_type": "code",
        "outputId": "ca03468a-5c82-453e-9839-4b87789c1d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "new_mapping2 = {}\n",
        "train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "cnt2 = 0\n",
        "for i in range(train_questions.shape[0]):\n",
        "  for j in range(train_questions.shape[1]):\n",
        "    v = train_questions[i][j]\n",
        "    if v not in new_mapping2:\n",
        "      new_mapping2[v] = cnt2\n",
        "      cnt2 += 1\n",
        "for i in range(val_questions.shape[0]):\n",
        "  for j in range(val_questions.shape[1]):\n",
        "    v = val_questions[i][j]\n",
        "    if v not in new_mapping2:\n",
        "      new_mapping2[v] = cnt2\n",
        "      cnt2 += 1\n",
        "print(cnt2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UI8M9SNUP4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_questions = np.load(\"new_filtered_val_questions.npy\")\n",
        "# for i in range(val_questions.shape[0]):\n",
        "#   for j in range(val_questions.shape[1]):\n",
        "#     if j!=0 and np.sum(val_questions[i,j:]) == 0:\n",
        "#       val_questions[i,j:] = 4344\n",
        "#       break\n",
        "\n",
        "# print(val_questions[:10])\n",
        "# np.save(\"new_filtered_val_questions.npy\", val_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjb84H5dgap7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "if model_input == \"resnet 18 image features, question\":\n",
        "  # train_answers = np.load(\"train_answers_\"+answer_type+\".npy\")\n",
        "  if Top100 == True:\n",
        "    train_indices = np.load(\"top100_train_indices.npy\")\n",
        "    val_indices = np.load(\"top100_val_indices.npy\")\n",
        "    \n",
        "  train_answers = np.load(\"train_answers.npy\")\n",
        "  if Top100 == True:\n",
        "    \n",
        "    train_answers = train_answers[train_indices]\n",
        "    answer_dict = {}\n",
        "    cnt = 0\n",
        "    print(train_answers.shape)\n",
        "    for i in range(train_answers.shape[0]):\n",
        "      v = train_answers[i][0]\n",
        "      if v not in answer_dict:\n",
        "        answer_dict[v] = cnt\n",
        "        cnt += 1\n",
        "    print(cnt)\n",
        "\n",
        "    print(answer_dict)\n",
        "    for i in range(train_answers.shape[0]):\n",
        "      train_answers[i,0] = answer_dict[train_answers[i,0]]\n",
        "    # train_answers = answer_dict[train_answers[:,0]]\n",
        "    # print(train_answers.shape)\n",
        "\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  # val_answers = np.load(\"val_answers_\"+answer_type+\".npy\")\n",
        "  val_answers = np.load(\"val_answers.npy\")\n",
        "  if Top100 == True:\n",
        "    val_answers = val_answers[val_indices]\n",
        "    for i in range(val_answers.shape[0]):\n",
        "      val_answers[i,0] = answer_dict[val_answers[i,0]]\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  \n",
        "elif model_input == \"resnet 192 image features, question\":\n",
        "  train_answers = np.load(\"train_answers_\"+answer_type+\".npy\")\n",
        "  # train_answers = np.load(\"train_answers.npy\")\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  val_answers = np.load(\"val_answers_\"+answer_type+\".npy\")\n",
        "  # val_answers = np.load(\"val_answers.npy\")\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  # train_indices = np.load(\"top100_train_indices.npy\")\n",
        "  # val_indices = np.load(\"top100_val_indices.npy\")\n",
        "elif model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "  train_answers = np.load(\"new_filtered_train_answers.npy\")\n",
        "  val_answers = np.load(\"new_filtered_val_answers.npy\")\n",
        "  # train_answers_copy = np.array(train_answers)\n",
        "  # train_answers = np.concatenate((train_answers_copy, train_answers_copy), axis=0)\n",
        "  # train_answers = train_answers[:13000]\n",
        "  # train_answers = train_answers[:2000]\n",
        "  # train_answers = np.concatenate((train_answers, val_answers[2600:]), axis=0)\n",
        "  answers_mapping = np.arange(0,104)\n",
        "  np.random.shuffle(answers_mapping)\n",
        "  new_train_answers = np.array(train_answers)\n",
        "  indices = np.arange(0,104)\n",
        "  for i in range(104):\n",
        "    ind = indices[indices==i]\n",
        "    new_train_answers[ind] = answers_mapping[i]\n",
        "  train_answers = new_train_answers\n",
        "  # train_answers = np.load(\"train_answers.npy\")\n",
        "  tensor_train_answers = torch.Tensor(train_answers).flatten()\n",
        "  tensor_train_answers = tensor_train_answers.type(torch.long)\n",
        "\n",
        "  \n",
        "  # val_answers = val_answers[:2600]\n",
        "  # val_answers = val_answers[:400]\n",
        "  new_train_answers = np.array(val_answers)\n",
        "  indices = np.arange(0,104)\n",
        "  for i in range(104):\n",
        "    ind = indices[indices==i]\n",
        "    new_train_answers[ind] = answers_mapping[i]\n",
        "  val_answers = new_train_answers\n",
        "  # val_answers = np.load(\"val_answers.npy\")\n",
        "  tensor_val_answers = torch.Tensor(val_answers).flatten()\n",
        "  tensor_val_answers = tensor_val_answers.type(torch.long)\n",
        "\n",
        "  # train_indices = np.load(\"top100_train_indices.npy\")\n",
        "  # val_indices = np.load(\"top100_val_indices.npy\")\n",
        "\n",
        "\n",
        "if model_input == \"resnet 18 image features, question\":\n",
        "  # train_images = np.load(\"train_image_features_\"+answer_type+\".npy\")\n",
        "  # train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  # val_images = np.load(\"val_image_features_\"+answer_type+\".npy\")\n",
        "  # val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  train_images = np.load(\"train_image_features.npy\")\n",
        "  train_questions = np.load(\"train_questions.npy\")\n",
        "  for i in range(train_questions.shape[0]):\n",
        "    for j in range(train_questions.shape[1]):\n",
        "      v = train_questions[i,j]\n",
        "      train_questions[i,j] = new_mapping[v]\n",
        "  # train_questions = train_questions[:,:20]\n",
        "\n",
        "  if Top100 == True:\n",
        "    train_images = train_images[train_indices]\n",
        "    train_questions = train_questions[train_indices]\n",
        "\n",
        "  val_images = np.load(\"val_image_features.npy\")\n",
        "  val_questions = np.load(\"val_questions.npy\")\n",
        "  for i in range(val_questions.shape[0]):\n",
        "    for j in range(val_questions.shape[1]):\n",
        "      v = val_questions[i,j]\n",
        "      val_questions[i,j] = new_mapping[v]\n",
        "\n",
        "  # val_questions = val_questions[:,:20]\n",
        "\n",
        "  if Top100 == True:\n",
        "    val_images = val_images[val_indices]\n",
        "    val_questions = val_questions[val_indices]\n",
        "\n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "elif model_input == \"resnet 192 image features, question\":\n",
        "  train_images = np.load(\"train_image_features_\"+answer_type+\".npy\")\n",
        "  train_questions = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "  val_images = np.load(\"val_image_features_\"+answer_type+\".npy\")\n",
        "  val_questions = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "  \n",
        "  for i in range(train_questions.shape[0]):\n",
        "    for j in range(train_questions.shape[1]):\n",
        "      v = train_questions[i,j]\n",
        "      train_questions[i,j] = new_mapping2[v]\n",
        "\n",
        "\n",
        "  for i in range(val_questions.shape[0]):\n",
        "    for j in range(val_questions.shape[1]):\n",
        "      v = val_questions[i,j]\n",
        "      val_questions[i,j] = new_mapping2[v]\n",
        "\n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "elif model_input == \"new resnet 18 features\":\n",
        "  train_images = np.load(\"new_filtered_train_image_features.npy\")\n",
        "  # train_images_copy = np.array(train_images)\n",
        "  # noise = np.random.normal(scale=2.0, size=[train_images.shape[0]*train_images.shape[1]])\n",
        "  # noise = np.reshape(noise, newshape=[train_images.shape[0], train_images.shape[1]])\n",
        "  # train_images_copy += noise\n",
        "  # train_images_a = np.array(train_images)\n",
        "  # train_images = np.concatenate((train_images_a, train_images_copy), axis=0)\n",
        "  # train_images = train_images[:13000]\n",
        "  # train_images = train_images[:2000]\n",
        "  train_questions = np.load(\"new_filtered_train_questions.npy\")\n",
        "  # train_questions_copy = np.array(train_questions)\n",
        "  # train_questions = np.concatenate((train_questions_copy, train_questions_copy), axis=0)\n",
        "  # train_questions = train_questions[:13000]\n",
        "  # train_questions = train_questions[:2000]\n",
        "\n",
        "  val_images = np.load(\"new_filtered_val_image_features.npy\")\n",
        "  \n",
        "  val_questions = np.load(\"new_filtered_val_questions.npy\")\n",
        "  \n",
        "\n",
        "  # print(train_images.shape)\n",
        "  # train_images = np.concatenate((train_images, val_images[2600:]), axis=0)\n",
        "  # print(train_images.shape)\n",
        "  # train_questions = np.concatenate((train_questions, val_questions[2600:]), axis=0)\n",
        "  # val_images = val_images[:2600]\n",
        "  # val_questions = val_questions[:2600]\n",
        "  # val_images = val_images[:400]\n",
        "  # val_questions = val_questions[:400]\n",
        "  \n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  # train_question_indices = np.zeros(shape=[train_questions.shape[0], 1])\n",
        "  # for i in range(train_questions.shape[0]):\n",
        "  #   for j in range(25):\n",
        "  #     if train_questions[i,j] == 4344:\n",
        "  #       train_question_indices[i,0] = j-1\n",
        "  #       break\n",
        "  # val_question_indices = np.zeros(shape=[val_questions.shape[0], 1])\n",
        "  # for i in range(val_questions.shape[0]):\n",
        "  #   for j in range(25):\n",
        "  #     if val_questions[i,j] == 4344:\n",
        "  #       val_question_indices[i,0] = j-1\n",
        "  #       break\n",
        "  \n",
        "  # np.save(\"new_filtered_train_question_indices.npy\", train_question_indices)\n",
        "  # np.save(\"new_filtered_val_question_indices.npy\", val_question_indices)\n",
        "\n",
        "  train_question_indices = np.load(\"new_filtered_train_question_indices.npy\")\n",
        "  # train_question_indices_copy = np.array(train_question_indices)\n",
        "  # train_question_indices = np.concatenate((train_question_indices_copy, train_question_indices_copy), axis=0)\n",
        "  # train_question_indices = train_question_indices[:13000]\n",
        "  # train_question_indices = train_question_indices[:2000]\n",
        "  val_question_indices = np.load(\"new_filtered_val_question_indices.npy\")\n",
        "  \n",
        "  # train_question_indices = np.concatenate((train_question_indices, val_question_indices[2600:]), axis=0)\n",
        "  # val_question_indices = val_question_indices[:2600]\n",
        "  # val_question_indices = val_question_indices[:400]\n",
        "\n",
        "  # train_question_indices[train_question_indices==-1] = 0\n",
        "  # val_question_indices[val_question_indices==-1] = 0\n",
        "\n",
        "  # np.save(\"new_filtered_train_question_indices.npy\", train_question_indices)\n",
        "  # np.save(\"new_filtered_val_question_indices.npy\", val_question_indices)\n",
        "\n",
        "\n",
        "  # print(train_question_indices[:10])\n",
        "  # print(train_questions[:10])\n",
        "  # print(val_question_indices[:10])\n",
        "  # print(val_questions[:10])\n",
        "  \n",
        "  # print(train_answers.shape)\n",
        "  # print(train_images.shape)\n",
        "  # print(train_questions.shape)\n",
        "  # print(train_question_indices.shape)\n",
        "\n",
        "  tensor_train_question_indices = torch.Tensor(train_question_indices)\n",
        "  tensor_val_question_indices = torch.Tensor(val_question_indices)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers, tensor_train_question_indices)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers, tensor_val_question_indices)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "elif model_input == \"new resnet 152 features\":\n",
        "  train_images = np.load(\"filtered_train_image_features_152.npy\")\n",
        "  train_questions = np.load(\"filtered_train_questions.npy\")\n",
        "\n",
        "  val_images = np.load(\"filtered_val_image_features_152.npy\")\n",
        "  val_questions = np.load(\"filtered_val_questions.npy\")\n",
        "  \n",
        "  tensor_train_images = torch.Tensor(train_images)\n",
        "  tensor_train_questions = torch.Tensor(train_questions)\n",
        "\n",
        "  tensor_val_images = torch.Tensor(val_images)\n",
        "  tensor_val_questions = torch.Tensor(val_questions)\n",
        "\n",
        "  trainset = data.TensorDataset(tensor_train_images,tensor_train_questions,tensor_train_answers)\n",
        "  valset = data.TensorDataset(tensor_val_images,tensor_val_questions,tensor_val_answers)\n",
        "\n",
        "  num_workers = 2\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4af-oe4XJlsy",
        "colab_type": "code",
        "outputId": "6fb2820a-cff0-4040-bc59-2fa5c000fefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "print(train_images.shape)\n",
        "print(val_images.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32498, 512)\n",
            "(20283, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7JuABKSJqSw",
        "colab_type": "code",
        "outputId": "1f113f6a-44f6-4c9b-ac9b-b52fae078ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_questions.shape)\n",
        "print(val_questions.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32498, 25)\n",
            "(20283, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mif2irg9KbaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57Wbz-44HBd",
        "colab_type": "text"
      },
      "source": [
        "#### Choose model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZFIWKQnz7om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model_type == \"simplecnnlstm\":\n",
        "  import models.Simple_CNN_LSTM_Model as models\n",
        "  # import models.trial_model as trial_model\n",
        "elif model_type == \"fusion\":\n",
        "  import models.fusion_model as fusion_model\n",
        "elif model_type == \"transformer\":\n",
        "  import models.transformer_model as transformer_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvpONlFiTMZk",
        "colab_type": "code",
        "outputId": "69a2be17-ae10-4e41-a68d-2d8fc115db5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(lstm_hidden_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynecbLrqOxk",
        "colab_type": "code",
        "outputId": "f5a0b5e6-2dc9-4ed8-d2ec-45907f742d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "weights_matrix = np.load(\"weights_matrix.npy\")\n",
        "weights_matrix = torch.tensor(weights_matrix)\n",
        "if model_type == \"simplecnnlstm\":\n",
        "  model = models.SimpleCNNLSTM(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, Top100=Top100)\n",
        "elif model_type == \"fusion\":\n",
        "  if answer_type == \"yesno\":\n",
        "    model = fusion_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=32, dropout_amount=dropout_amount, Top100=Top100)\n",
        "  else:\n",
        "    model = fusion_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=256, dropout_amount=dropout_amount, Top100=Top100)\n",
        "elif model_type == \"transformer\":\n",
        "  if answer_type == \"yesno\":\n",
        "    model = transformer_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=32, dropout_amount=dropout_amount, Top100=Top100)\n",
        "  else:\n",
        "    model = transformer_model.FusionModel(weights_matrix, lstm_hidden_size, lstm_num_layers, model_input, answer_type, fusion_type, shared_size=256, dropout_amount=dropout_amount, Top100=Top100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bU4rv0_V12s",
        "colab_type": "code",
        "outputId": "4f48ddcb-e864-4438-98e2-769a8443fede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FusionModel(\n",
            "  (lstm_model): SimpleLSTM(\n",
            "    (embedding): Embedding(4400, 50)\n",
            "    (gru): GRU(50, 64, batch_first=True, dropout=0.3)\n",
            "  )\n",
            "  (img_dense1): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (img_dense2): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (q_dense1): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (q_dense2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=256, out_features=104, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dS4E_c-4dp_",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_czxJkrlwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "lr_reducer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=np.sqrt(0.81),\n",
        "                                                                        cooldown=0,\n",
        "                                                                        patience=3,\n",
        "                                                                        min_lr=0.5e-15, mode='min', verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUsFk8z_2XJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(predictions, labels):\n",
        "  predictions = torch.max(predictions, axis=1)[1]\n",
        "  ab = torch.abs(predictions-labels)\n",
        "  ab = ab.detach().numpy()\n",
        "  mn = np.minimum(ab, 1)\n",
        "  eq = 1-mn\n",
        "  correct = np.sum(eq)\n",
        "  total = eq.shape[0]\n",
        "  return correct, total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUmF088VgUuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(y_pred, y_true, gamma=2., alpha=4.):\n",
        "\n",
        "    gamma = float(gamma)\n",
        "    alpha = float(alpha)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Focal loss for multi-classification\n",
        "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
        "        Notice: y_pred is probability after softmax\n",
        "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
        "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
        "        Focal Loss for Dense Object Detection\n",
        "        https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Arguments:\n",
        "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
        "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
        "\n",
        "        Keyword Arguments:\n",
        "            gamma {float} -- (default: {2.0})\n",
        "            alpha {float} -- (default: {4.0})\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- loss.\n",
        "        \"\"\"\n",
        "        # y_pred = y_pred.detach().numpy()\n",
        "        # y_pred = np.array(y_pred, dtype=np.int32)\n",
        "        # print(y_true.shape)\n",
        "        # print(y_pred.shape)\n",
        "        # y_pred_onehot = np.zeros(shape=[y_pred.shape[0], y_true.size(1)])\n",
        "        # y_pred_onehot[:,y_pred] = 1\n",
        "        # if answer_type != \"yesno\":\n",
        "        y_true_onehot = torch.FloatTensor(y_pred.size())\n",
        "        y_true_onehot.zero_()\n",
        "        y_true_onehot.scatter_(1, y_true.type(torch.long).view(y_true.size(0),1), 1.0)\n",
        "        y_true = y_true_onehot\n",
        "\n",
        "        epsilon = 1.e-9\n",
        "        # y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
        "        # y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
        "        y_pred = y_pred.type(torch.float32)\n",
        "        y_true = y_true.type(torch.float32)\n",
        "\n",
        "        # print(torch.max(y_true[:10], axis=1))\n",
        "        # print(torch.max(y_pred[:10], axis=1))\n",
        "\n",
        "        model_out = torch.add(y_pred, epsilon)\n",
        "        # print(\"model_out: \",model_out[:1])\n",
        "        # model_out = tf.add(y_pred, epsilon)\n",
        "        ce = torch.mul(y_true, torch.log(model_out))\n",
        "        # print(\"ce: \",ce[:1])\n",
        "        # ce = tf.multiply(y_true, -tf.log(model_out))\n",
        "        # print(\"1-model_out: \",(1-model_out)[:1])\n",
        "        # print(\"gamma: \",gamma)\n",
        "        # print(\"pow: \",torch.pow((1-model_out), gamma)[:1])\n",
        "        weight = torch.mul(y_true, torch.pow((1-model_out), gamma))\n",
        "        # print(\"weight: \",weight[:1])\n",
        "        # weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
        "        fl = -1*torch.mul(torch.mul(alpha, weight), ce)\n",
        "        # print(\"fl: \",fl[:1])\n",
        "        # fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
        "        reduced_fl = torch.max(fl, axis=1)[0]\n",
        "        # print(\"reduced_fl: \",reduced_fl[:1])\n",
        "        # reduced_fl = tf.reduce_max(fl, axis=1)\n",
        "        return torch.mean(reduced_fl)\n",
        "        # return tf.reduce_mean(reduced_fl)\n",
        "    loss = focal_loss_fixed(y_true, y_pred)\n",
        "    # print(loss)\n",
        "    return loss\n",
        "\n",
        "alpha = 4.\n",
        "gamma = 2."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcEf05wAmYap",
        "colab_type": "code",
        "outputId": "b7c2fda4-d8cb-4081-b68e-54ac7da6efbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    num_batches = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "    # for i, data in enumerate(valloader, 0):\n",
        "        # if i % print_every == 0 and i>0:\n",
        "          # print(\"Train batch \",i+1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "          image_features, questions, labels, q_indices = data\n",
        "          \n",
        "          # print(image_features[0])\n",
        "          # print(questions[0].detach().numpy().astype(np.int32))\n",
        "          # print(labels[0])\n",
        "          # break\n",
        "          # print(labels.size())\n",
        "          # questions = questions/3100\n",
        "          # questions = questions[:,:5]\n",
        "          image_features = image_features.type(torch.float32)\n",
        "          labels = labels.flatten().type(torch.long)\n",
        "          labels = labels.view(-1)\n",
        "\n",
        "          outputs = model(image_features, questions, q_indices)\n",
        "          # print(\"out done\")\n",
        "\n",
        "        \n",
        "        batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "        # print(\"accuracy done\")\n",
        "        correct += batch_correct\n",
        "        total += batch_total\n",
        "        loss = focal_loss(outputs, labels, alpha=alpha, gamma=gamma)\n",
        "        # loss = criterion(outputs, labels)\n",
        "        # print(\"loss done\")\n",
        "        loss.backward()\n",
        "        # print(\"backprop done\")\n",
        "        optimizer.step()\n",
        "        # print(\"optim done\")\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_loss += running_loss\n",
        "        num_batches += 1\n",
        "        running_loss = 0.0\n",
        "\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_batches = 0\n",
        "    val_total_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      # for i, data in enumerate(trainloader, 0):\n",
        "      for i, data in enumerate(valloader, 0):\n",
        "          # if i % print_every == 0 and i>0:\n",
        "            # print(\"Val batch \",i+1)\n",
        "\n",
        "          if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "            image_features, questions, labels, q_indices = data\n",
        "            # questions = questions/3100\n",
        "            # questions = questions[:,:5]\n",
        "            image_features = image_features.type(torch.float32)\n",
        "            labels = labels.flatten().type(torch.long)\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            outputs = model(image_features, questions, q_indices)\n",
        "\n",
        "          batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "          val_correct += batch_correct\n",
        "          val_total += batch_total\n",
        "          val_batches += 1\n",
        "\n",
        "          val_loss = focal_loss(outputs, labels, alpha=alpha, gamma=gamma)\n",
        "          # val_loss = criterion(outputs, labels)\n",
        "          val_total_loss += val_loss.detach().numpy()\n",
        "          \n",
        "\n",
        "    lr_reducer.step(val_total_loss/val_batches)\n",
        "    # lr_reducer.step(val_total_loss)\n",
        "\n",
        "    with open(\"output/train_loss.txt\",\"a\") as f:\n",
        "        f.write(str(total_loss))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "   \n",
        "    with open(\"output/val_loss.txt\",\"a\") as f:\n",
        "        f.write(str(val_total_loss))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    with open(\"output/train_accuracy.txt\",\"a\") as f:\n",
        "        f.write(str(correct/total))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "   \n",
        "    with open(\"output/val_accuracy.txt\",\"a\") as f:\n",
        "        f.write(str(val_correct/val_total))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "    torch.save(model.state_dict(),\"output/model.pt\")\n",
        "\n",
        "    with open(\"output/output.txt\",\"a\") as f:\n",
        "        f.write(\"Epoch: \"+str(epoch+1)+\" Loss: \"+str(total_loss/num_batches)+\" Train-Accuracy: \"+ str(correct/total)+\" Val Loss: \"+str(val_total_loss/val_batches)+\" Val-Accuracy: \"+str(val_correct/val_total))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "    print(\"Epoch: \",epoch+1,\" Loss: \",total_loss/num_batches,\" Train-Accuracy: \", correct/total,\" Val Loss: \",val_total_loss/val_batches,\" Val-Accuracy: \",val_correct/val_total)\n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1  Loss:  18.496444297559332  Train-Accuracy:  0.027817096436703798  Val Loss:  17.451953070504324  Val-Accuracy:  0.08144751762559779\n",
            "Epoch:  2  Loss:  16.942282821192887  Train-Accuracy:  0.08458982091205612  Val Loss:  15.996207191830589  Val-Accuracy:  0.1358280333284031\n",
            "Epoch:  3  Loss:  15.851884119438402  Train-Accuracy:  0.1289925533879008  Val Loss:  15.195779028392973  Val-Accuracy:  0.17004387911058522\n",
            "Epoch:  4  Loss:  14.921103304082697  Train-Accuracy:  0.16016370238168504  Val Loss:  14.31425580524263  Val-Accuracy:  0.1957797170043879\n",
            "Epoch:  5  Loss:  13.655297828443123  Train-Accuracy:  0.19739676287771554  Val Loss:  13.058456511724563  Val-Accuracy:  0.22541044224227186\n",
            "Epoch:  6  Loss:  12.05746957027551  Train-Accuracy:  0.2285371407471229  Val Loss:  11.779238927932013  Val-Accuracy:  0.2579993097667998\n",
            "Epoch:  7  Loss:  10.677680044463187  Train-Accuracy:  0.25552341682565083  Val Loss:  10.825287001473564  Val-Accuracy:  0.287087708918799\n",
            "Epoch:  8  Loss:  9.64836499185273  Train-Accuracy:  0.2851560096005908  Val Loss:  10.025356883094425  Val-Accuracy:  0.3042942365527782\n",
            "Epoch:  9  Loss:  8.832138148221103  Train-Accuracy:  0.3024801526247769  Val Loss:  9.5134372257051  Val-Accuracy:  0.3215007641867574\n",
            "Epoch:  10  Loss:  8.219687230659254  Train-Accuracy:  0.3139577820173549  Val Loss:  9.04993352435884  Val-Accuracy:  0.33224868116156386\n",
            "Epoch:  11  Loss:  7.724261038231127  Train-Accuracy:  0.33728229429503354  Val Loss:  8.643818719046456  Val-Accuracy:  0.35167381551052607\n",
            "Epoch:  12  Loss:  7.3395232287320225  Train-Accuracy:  0.3488522370607422  Val Loss:  8.359631674630302  Val-Accuracy:  0.3579352166839225\n",
            "Epoch:  13  Loss:  7.066684303861676  Train-Accuracy:  0.3559603667918026  Val Loss:  8.122890177227202  Val-Accuracy:  0.3683380170586205\n",
            "Epoch:  14  Loss:  6.770161224134041  Train-Accuracy:  0.3705458797464459  Val Loss:  7.835551488967169  Val-Accuracy:  0.379825469605088\n",
            "Epoch:  15  Loss:  6.529082543922193  Train-Accuracy:  0.37857714320881286  Val Loss:  7.62490109034947  Val-Accuracy:  0.3903761770941182\n",
            "Epoch:  16  Loss:  6.284495151404179  Train-Accuracy:  0.3903624838451597  Val Loss:  7.473110970996675  Val-Accuracy:  0.3921017600946606\n",
            "Epoch:  17  Loss:  6.152655717098352  Train-Accuracy:  0.40180934211336083  Val Loss:  7.347793942406064  Val-Accuracy:  0.39782083518217226\n",
            "Epoch:  18  Loss:  5.974874453111128  Train-Accuracy:  0.4084866761031448  Val Loss:  7.268292222704206  Val-Accuracy:  0.4021594438692501\n",
            "Epoch:  19  Loss:  5.863215518720223  Train-Accuracy:  0.4120868976552403  Val Loss:  7.171608516148159  Val-Accuracy:  0.4033920031553518\n",
            "Epoch:  20  Loss:  5.7012496572552305  Train-Accuracy:  0.4221490553264816  Val Loss:  7.135634853726342  Val-Accuracy:  0.40600502884188727\n",
            "Epoch:  21  Loss:  5.542634053663774  Train-Accuracy:  0.4261800726198535  Val Loss:  7.0628017243884855  Val-Accuracy:  0.40916038061430754\n",
            "Epoch:  22  Loss:  5.444722233396588  Train-Accuracy:  0.433534371345929  Val Loss:  6.909468537285214  Val-Accuracy:  0.41226643001528374\n",
            "Epoch:  23  Loss:  5.3417917887369795  Train-Accuracy:  0.43975013847005967  Val Loss:  6.980806100936163  Val-Accuracy:  0.41182270867228715\n",
            "Epoch:  24  Loss:  5.251063708103064  Train-Accuracy:  0.44331958889777834  Val Loss:  6.876601491655622  Val-Accuracy:  0.41635852684514124\n",
            "Epoch:  25  Loss:  5.162880391785593  Train-Accuracy:  0.4455658809772909  Val Loss:  6.837745870862689  Val-Accuracy:  0.41887294778878864\n",
            "Epoch:  26  Loss:  5.080740148370916  Train-Accuracy:  0.4498738383900548  Val Loss:  6.763350918179467  Val-Accuracy:  0.4224720209042055\n",
            "Epoch:  27  Loss:  4.9722888686440205  Train-Accuracy:  0.45839743984245185  Val Loss:  6.773714928399949  Val-Accuracy:  0.4238031849331953\n",
            "Epoch:  28  Loss:  4.8955919814832285  Train-Accuracy:  0.4631977352452459  Val Loss:  6.843000366574242  Val-Accuracy:  0.4238031849331953\n",
            "Epoch:  29  Loss:  4.849199757431492  Train-Accuracy:  0.46519785832974336  Val Loss:  6.80600863411313  Val-Accuracy:  0.42897993393482226\n",
            "Epoch    30: reducing learning rate of group 0 to 9.0000e-04.\n",
            "Epoch:  30  Loss:  4.772892677422726  Train-Accuracy:  0.47107514308572834  Val Loss:  6.7973019282023115  Val-Accuracy:  0.4268599319627274\n",
            "Epoch:  31  Loss:  4.706522117961537  Train-Accuracy:  0.4709520585882208  Val Loss:  6.779363268897647  Val-Accuracy:  0.43109993590691714\n",
            "Epoch:  32  Loss:  4.599024093512333  Train-Accuracy:  0.48046033602067817  Val Loss:  6.758127575828915  Val-Accuracy:  0.4312478430212493\n",
            "Epoch:  33  Loss:  4.558476390260639  Train-Accuracy:  0.48289125484645207  Val Loss:  6.71398959841047  Val-Accuracy:  0.43780505842331013\n",
            "Epoch:  34  Loss:  4.486717802105528  Train-Accuracy:  0.48415287094590437  Val Loss:  6.715985434395926  Val-Accuracy:  0.43598087067987973\n",
            "Epoch:  35  Loss:  4.426124789498069  Train-Accuracy:  0.49212259215951754  Val Loss:  6.718684786841983  Val-Accuracy:  0.43484691613666615\n",
            "Epoch:  36  Loss:  4.405702966632265  Train-Accuracy:  0.4935688350052311  Val Loss:  6.725598199026925  Val-Accuracy:  0.4296701671350392\n",
            "Epoch:  37  Loss:  4.3342289418885205  Train-Accuracy:  0.4967382608160502  Val Loss:  6.703502677735829  Val-Accuracy:  0.43060691219247643\n",
            "Epoch:  38  Loss:  4.305079944206007  Train-Accuracy:  0.4975383100498492  Val Loss:  6.659171195257278  Val-Accuracy:  0.43440319479366957\n",
            "Epoch:  39  Loss:  4.235810944528291  Train-Accuracy:  0.5043387285371408  Val Loss:  6.685136726924351  Val-Accuracy:  0.4347976137652221\n",
            "Epoch:  40  Loss:  4.216144106604836  Train-Accuracy:  0.5017231829651055  Val Loss:  6.709914707002186  Val-Accuracy:  0.4342059853078933\n",
            "Epoch:  41  Loss:  4.181480039249767  Train-Accuracy:  0.5069542741091759  Val Loss:  6.699775559561593  Val-Accuracy:  0.4341073805650052\n",
            "Epoch    42: reducing learning rate of group 0 to 8.1000e-04.\n",
            "Epoch:  42  Loss:  4.109083739194003  Train-Accuracy:  0.5115699427657087  Val Loss:  6.706230481465657  Val-Accuracy:  0.4358822659369916\n",
            "Epoch:  43  Loss:  4.061245557033654  Train-Accuracy:  0.5132315834820604  Val Loss:  6.7184295654296875  Val-Accuracy:  0.4368683133658729\n",
            "Epoch:  44  Loss:  4.017164317044345  Train-Accuracy:  0.5164010092928796  Val Loss:  6.745638915470669  Val-Accuracy:  0.4373120347088695\n",
            "Epoch:  45  Loss:  3.9454730929750386  Train-Accuracy:  0.5220936673026032  Val Loss:  6.737244787670317  Val-Accuracy:  0.4393827343095203\n",
            "Epoch    46: reducing learning rate of group 0 to 7.2900e-04.\n",
            "Epoch:  46  Loss:  3.9302659684961494  Train-Accuracy:  0.5268016493322666  Val Loss:  6.735707555498395  Val-Accuracy:  0.44027017699551346\n",
            "Epoch:  47  Loss:  3.891131726178256  Train-Accuracy:  0.5255092621084374  Val Loss:  6.748217877887544  Val-Accuracy:  0.438396686880639\n",
            "Epoch:  48  Loss:  3.846527150183013  Train-Accuracy:  0.5302480152624777  Val Loss:  6.7786004429771785  Val-Accuracy:  0.43815017502341863\n",
            "Epoch:  49  Loss:  3.8180256178884795  Train-Accuracy:  0.5324943073419903  Val Loss:  6.738462084815616  Val-Accuracy:  0.4377557560518661\n",
            "Epoch    50: reducing learning rate of group 0 to 6.5610e-04.\n",
            "Epoch:  50  Loss:  3.7608603275183476  Train-Accuracy:  0.5383408209735984  Val Loss:  6.7835943812415715  Val-Accuracy:  0.44160134102450327\n",
            "Epoch:  51  Loss:  3.7449726191433994  Train-Accuracy:  0.5395716659486738  Val Loss:  6.744080725170317  Val-Accuracy:  0.4405166888527338\n",
            "Epoch:  52  Loss:  3.7002365733637954  Train-Accuracy:  0.538710074466121  Val Loss:  6.7862698918297175  Val-Accuracy:  0.4384952916235271\n",
            "Epoch:  53  Loss:  3.7076927170608984  Train-Accuracy:  0.5421256692719552  Val Loss:  6.736247562226795  Val-Accuracy:  0.4422915742247202\n",
            "Epoch    54: reducing learning rate of group 0 to 5.9049e-04.\n",
            "Epoch:  54  Loss:  3.6498018900553384  Train-Accuracy:  0.5452950950827743  Val Loss:  6.842489810216994  Val-Accuracy:  0.44194645762461177\n",
            "Epoch:  55  Loss:  3.591929276784261  Train-Accuracy:  0.5492953412517694  Val Loss:  6.844625677381243  Val-Accuracy:  0.442883202682049\n",
            "Epoch:  56  Loss:  3.6077250350605357  Train-Accuracy:  0.5452335528340205  Val Loss:  6.829072770618257  Val-Accuracy:  0.4432776216536015\n",
            "Epoch:  57  Loss:  3.543245994683468  Train-Accuracy:  0.5546495168933473  Val Loss:  6.792684078216553  Val-Accuracy:  0.44529901888280826\n",
            "Epoch    58: reducing learning rate of group 0 to 5.3144e-04.\n",
            "Epoch:  58  Loss:  3.5207806067033247  Train-Accuracy:  0.5578497138285433  Val Loss:  6.902876445225307  Val-Accuracy:  0.4418478528817236\n",
            "Epoch:  59  Loss:  3.478669535030018  Train-Accuracy:  0.5619422733706689  Val Loss:  6.8640791575113935  Val-Accuracy:  0.44515111176847605\n",
            "Epoch:  60  Loss:  3.4642113122073086  Train-Accuracy:  0.5600036925349252  Val Loss:  6.79534794035412  Val-Accuracy:  0.44574274022580485\n",
            "Epoch:  61  Loss:  3.447177048885461  Train-Accuracy:  0.5615114776293926  Val Loss:  6.869217373075939  Val-Accuracy:  0.44204506236749985\n",
            "Epoch    62: reducing learning rate of group 0 to 4.7830e-04.\n",
            "Epoch:  62  Loss:  3.438191218809648  Train-Accuracy:  0.565696350544649  Val Loss:  6.850514184860956  Val-Accuracy:  0.44470739042547947\n",
            "Epoch:  63  Loss:  3.3922963359139184  Train-Accuracy:  0.564619361191458  Val Loss:  6.855816614060175  Val-Accuracy:  0.4461864615688015\n",
            "Epoch:  64  Loss:  3.3884665460297554  Train-Accuracy:  0.5679426426241615  Val Loss:  6.8902604920523505  Val-Accuracy:  0.44204506236749985\n",
            "Epoch:  65  Loss:  3.367575175834425  Train-Accuracy:  0.5700350790817896  Val Loss:  6.942068576812744  Val-Accuracy:  0.4442636690824829\n",
            "Epoch    66: reducing learning rate of group 0 to 4.3047e-04.\n",
            "Epoch:  66  Loss:  3.336142554427638  Train-Accuracy:  0.5723736845344329  Val Loss:  6.955451147896903  Val-Accuracy:  0.4462357639402455\n",
            "Epoch:  67  Loss:  3.323699134768862  Train-Accuracy:  0.5679426426241615  Val Loss:  6.893161728268578  Val-Accuracy:  0.4445101809397032\n",
            "Epoch:  68  Loss:  3.2955464088555537  Train-Accuracy:  0.5770201243153424  Val Loss:  6.94172157560076  Val-Accuracy:  0.445841344968693\n",
            "Epoch:  69  Loss:  3.2786230968706533  Train-Accuracy:  0.5744661209920611  Val Loss:  6.9305219650268555  Val-Accuracy:  0.44396785485381846\n",
            "Epoch    70: reducing learning rate of group 0 to 3.8742e-04.\n",
            "Epoch:  70  Loss:  3.2763321038448447  Train-Accuracy:  0.5752969413502369  Val Loss:  6.974609783717564  Val-Accuracy:  0.44416506433959474\n",
            "Epoch:  71  Loss:  3.243003679044319  Train-Accuracy:  0.5796664410117546  Val Loss:  6.9468978927249  Val-Accuracy:  0.4462357639402455\n",
            "Epoch:  72  Loss:  3.1839312929095644  Train-Accuracy:  0.5832666625638501  Val Loss:  6.985293206714449  Val-Accuracy:  0.4471725089976828\n",
            "Epoch:  73  Loss:  3.2064168380968496  Train-Accuracy:  0.5829589513200812  Val Loss:  6.976737839835031  Val-Accuracy:  0.4456441354829167\n",
            "Epoch    74: reducing learning rate of group 0 to 3.4868e-04.\n",
            "Epoch:  74  Loss:  3.1915071588574033  Train-Accuracy:  0.5829281801957044  Val Loss:  7.009455748966762  Val-Accuracy:  0.4442143667110388\n",
            "Epoch:  75  Loss:  3.1934039664990976  Train-Accuracy:  0.5836359160563727  Val Loss:  6.981777440933954  Val-Accuracy:  0.44396785485381846\n",
            "Epoch:  76  Loss:  3.1728087627526484  Train-Accuracy:  0.5845590497876793  Val Loss:  6.965004080817813  Val-Accuracy:  0.44751762559779124\n",
            "Epoch:  77  Loss:  3.1416942206296055  Train-Accuracy:  0.5857591236383778  Val Loss:  6.959845906212216  Val-Accuracy:  0.4450032046541439\n",
            "Epoch    78: reducing learning rate of group 0 to 3.1381e-04.\n",
            "Epoch:  78  Loss:  3.136318539128159  Train-Accuracy:  0.5865899439965536  Val Loss:  6.991246995471773  Val-Accuracy:  0.4476162303406794\n",
            "Epoch:  79  Loss:  3.122706969579061  Train-Accuracy:  0.5916364083943627  Val Loss:  7.057820842379615  Val-Accuracy:  0.44648227579746586\n",
            "Epoch:  80  Loss:  3.105308200373794  Train-Accuracy:  0.5900055388023878  Val Loss:  7.0376377786908835  Val-Accuracy:  0.4468273923975743\n",
            "Epoch:  81  Loss:  3.1060796650973233  Train-Accuracy:  0.5878515600960059  Val Loss:  7.013787837255569  Val-Accuracy:  0.44648227579746586\n",
            "Epoch    82: reducing learning rate of group 0 to 2.8243e-04.\n",
            "Epoch:  82  Loss:  3.076283339298133  Train-Accuracy:  0.5900670810511416  Val Loss:  6.971788974035354  Val-Accuracy:  0.44746832322634716\n",
            "Epoch:  83  Loss:  3.0456714991367226  Train-Accuracy:  0.5967444150409256  Val Loss:  7.03893332254319  Val-Accuracy:  0.44687669476901837\n",
            "Epoch:  84  Loss:  3.054463191465898  Train-Accuracy:  0.594775063080805  Val Loss:  7.018075125558036  Val-Accuracy:  0.44840506828378446\n",
            "Epoch:  85  Loss:  3.045218258193045  Train-Accuracy:  0.595852052433996  Val Loss:  7.111283188774472  Val-Accuracy:  0.4470246018833506\n",
            "Epoch    86: reducing learning rate of group 0 to 2.5419e-04.\n",
            "Epoch:  86  Loss:  3.0440156893296675  Train-Accuracy:  0.5932365068619607  Val Loss:  7.059712273733957  Val-Accuracy:  0.4459892520830252\n",
            "Epoch:  87  Loss:  3.0586460070176558  Train-Accuracy:  0.5946212074589206  Val Loss:  7.0112599191211515  Val-Accuracy:  0.44746832322634716\n",
            "Epoch:  88  Loss:  2.995787844513402  Train-Accuracy:  0.5988676226229307  Val Loss:  7.053370952606201  Val-Accuracy:  0.44781343982645566\n",
            "Epoch:  89  Loss:  2.9911325844851406  Train-Accuracy:  0.5993599606129608  Val Loss:  7.127397242046538  Val-Accuracy:  0.44727111374057094\n",
            "Epoch    90: reducing learning rate of group 0 to 2.2877e-04.\n",
            "Epoch:  90  Loss:  2.9502951376365893  Train-Accuracy:  0.6039448581451167  Val Loss:  7.09072664805821  Val-Accuracy:  0.44786274219789973\n",
            "Epoch:  91  Loss:  2.988018830617269  Train-Accuracy:  0.6002523232198904  Val Loss:  7.1197931198846724  Val-Accuracy:  0.4490953014840014\n",
            "Epoch:  92  Loss:  2.9770912835092256  Train-Accuracy:  0.6016370238168502  Val Loss:  7.091190224602109  Val-Accuracy:  0.44884878962678104\n",
            "Epoch:  93  Loss:  2.958836237589518  Train-Accuracy:  0.604652594005785  Val Loss:  7.095193136306036  Val-Accuracy:  0.44746832322634716\n",
            "Epoch    94: reducing learning rate of group 0 to 2.0589e-04.\n",
            "Epoch:  94  Loss:  2.9601817709026914  Train-Accuracy:  0.6018524216874884  Val Loss:  7.086592583429246  Val-Accuracy:  0.44924320859833355\n",
            "Epoch:  95  Loss:  2.931480393265233  Train-Accuracy:  0.6068988860852975  Val Loss:  7.107003439040411  Val-Accuracy:  0.4477148350835675\n",
            "Epoch:  96  Loss:  2.9376794569420093  Train-Accuracy:  0.6048987630008  Val Loss:  7.09076434090024  Val-Accuracy:  0.4505250702558793\n",
            "Epoch:  97  Loss:  2.913691585714167  Train-Accuracy:  0.6085605268016493  Val Loss:  7.1317877769470215  Val-Accuracy:  0.4477641374550116\n",
            "Epoch    98: reducing learning rate of group 0 to 1.8530e-04.\n",
            "Epoch:  98  Loss:  2.906485839323564  Train-Accuracy:  0.6060988368514986  Val Loss:  7.10573471160162  Val-Accuracy:  0.4492925109697776\n",
            "Epoch:  99  Loss:  2.9397682854623506  Train-Accuracy:  0.6048372207520463  Val Loss:  7.12614057177589  Val-Accuracy:  0.44786274219789973\n",
            "Epoch:  100  Loss:  2.9060328512480766  Train-Accuracy:  0.6082835866822573  Val Loss:  7.075378690447126  Val-Accuracy:  0.4489473943696692\n",
            "Epoch:  101  Loss:  2.9071468512217202  Train-Accuracy:  0.6087143824235337  Val Loss:  7.16504769098191  Val-Accuracy:  0.44825716116945225\n",
            "Epoch   102: reducing learning rate of group 0 to 1.6677e-04.\n",
            "Epoch:  102  Loss:  2.9011746897841943  Train-Accuracy:  0.6086528401747799  Val Loss:  7.149009954361689  Val-Accuracy:  0.4479120445693438\n",
            "Epoch:  103  Loss:  2.865167574449019  Train-Accuracy:  0.6126223152193981  Val Loss:  7.078893320901053  Val-Accuracy:  0.4490459991125573\n",
            "Epoch:  104  Loss:  2.883592627265237  Train-Accuracy:  0.6115145547418303  Val Loss:  7.140725476401193  Val-Accuracy:  0.44998274416999456\n",
            "Epoch:  105  Loss:  2.86669727527734  Train-Accuracy:  0.6120068927318604  Val Loss:  7.103458404541016  Val-Accuracy:  0.4467287876546862\n",
            "Epoch   106: reducing learning rate of group 0 to 1.5009e-04.\n",
            "Epoch:  106  Loss:  2.8452143813624526  Train-Accuracy:  0.6138839313188504  Val Loss:  7.179172402336484  Val-Accuracy:  0.4470246018833506\n",
            "Epoch:  107  Loss:  2.860199039632624  Train-Accuracy:  0.6128684842144132  Val Loss:  7.133401643662226  Val-Accuracy:  0.4488980919982251\n",
            "Epoch:  108  Loss:  2.852431889736291  Train-Accuracy:  0.6108991322542926  Val Loss:  7.147754532950265  Val-Accuracy:  0.4489473943696692\n",
            "Epoch:  109  Loss:  2.8772627223621714  Train-Accuracy:  0.608929780294172  Val Loss:  7.13052545275007  Val-Accuracy:  0.44593994971158113\n",
            "Epoch   110: reducing learning rate of group 0 to 1.3509e-04.\n",
            "Epoch:  110  Loss:  2.8368137460766416  Train-Accuracy:  0.6113606991199458  Val Loss:  7.220425242469425  Val-Accuracy:  0.4493418133412217\n",
            "Epoch:  111  Loss:  2.8601424332821006  Train-Accuracy:  0.616130223398363  Val Loss:  7.1225836390540715  Val-Accuracy:  0.44879948725533697\n",
            "Epoch:  112  Loss:  2.8215041810815986  Train-Accuracy:  0.6178534063634685  Val Loss:  7.167795045035226  Val-Accuracy:  0.4474190208549031\n",
            "Epoch:  113  Loss:  2.806742299686779  Train-Accuracy:  0.6144378115576343  Val Loss:  7.128474280947731  Val-Accuracy:  0.44781343982645566\n",
            "Epoch   114: reducing learning rate of group 0 to 1.2158e-04.\n",
            "Epoch:  114  Loss:  2.8166445818814365  Train-Accuracy:  0.6184072866022524  Val Loss:  7.226272900899251  Val-Accuracy:  0.4473697184834591\n",
            "Epoch:  115  Loss:  2.8003987832502886  Train-Accuracy:  0.6180380331097298  Val Loss:  7.1770601499648325  Val-Accuracy:  0.45077158211309964\n",
            "Epoch:  116  Loss:  2.8352126641706987  Train-Accuracy:  0.6197304449504585  Val Loss:  7.190329324631464  Val-Accuracy:  0.44825716116945225\n",
            "Epoch:  117  Loss:  2.829755197871815  Train-Accuracy:  0.614530124930765  Val Loss:  7.1974834487551735  Val-Accuracy:  0.449539022826998\n",
            "Epoch   118: reducing learning rate of group 0 to 1.0942e-04.\n",
            "Epoch:  118  Loss:  2.7774733846837822  Train-Accuracy:  0.6204381808111269  Val Loss:  7.158578236897786  Val-Accuracy:  0.44998274416999456\n",
            "Epoch:  119  Loss:  2.7965846711939033  Train-Accuracy:  0.6153301741645639  Val Loss:  7.205758117494129  Val-Accuracy:  0.4498841394271064\n",
            "Epoch:  120  Loss:  2.7882395802122173  Train-Accuracy:  0.6175456951196997  Val Loss:  7.175724915095738  Val-Accuracy:  0.4489473943696692\n",
            "Epoch:  121  Loss:  2.77401979041822  Train-Accuracy:  0.6226844728906394  Val Loss:  7.183550516764323  Val-Accuracy:  0.4501306512843268\n",
            "Epoch   122: reducing learning rate of group 0 to 9.8477e-05.\n",
            "Epoch:  122  Loss:  2.7739644700830635  Train-Accuracy:  0.6211766877961721  Val Loss:  7.173228649866013  Val-Accuracy:  0.4490459991125573\n",
            "Epoch:  123  Loss:  2.760035160816077  Train-Accuracy:  0.6232691242538002  Val Loss:  7.211632751283192  Val-Accuracy:  0.44939111571266577\n",
            "Epoch:  124  Loss:  2.7885194763992773  Train-Accuracy:  0.6218536525324635  Val Loss:  7.218042691548665  Val-Accuracy:  0.4483557659123404\n",
            "Epoch:  125  Loss:  2.7819646922024814  Train-Accuracy:  0.6230229552587851  Val Loss:  7.200164499736967  Val-Accuracy:  0.44692599714046244\n",
            "Epoch   126: reducing learning rate of group 0 to 8.8629e-05.\n",
            "Epoch:  126  Loss:  2.770269133827903  Train-Accuracy:  0.6193611914579359  Val Loss:  7.215093181246803  Val-Accuracy:  0.44746832322634716\n",
            "Epoch:  127  Loss:  2.766529401143392  Train-Accuracy:  0.6187765400947751  Val Loss:  7.225392704918271  Val-Accuracy:  0.44870088251244883\n",
            "Epoch:  128  Loss:  2.764560692238085  Train-Accuracy:  0.6219459659055941  Val Loss:  7.226768334706624  Val-Accuracy:  0.44944041808410984\n",
            "Epoch:  129  Loss:  2.756465117136637  Train-Accuracy:  0.6171148993784233  Val Loss:  7.189746538798015  Val-Accuracy:  0.4498841394271064\n",
            "Epoch   130: reducing learning rate of group 0 to 7.9766e-05.\n",
            "Epoch:  130  Loss:  2.771436330043908  Train-Accuracy:  0.6192381069604284  Val Loss:  7.207040514264788  Val-Accuracy:  0.4481585564265641\n",
            "Epoch:  131  Loss:  2.7689876267404268  Train-Accuracy:  0.6209920610499108  Val Loss:  7.184785979134696  Val-Accuracy:  0.4482078587980082\n",
            "Epoch:  132  Loss:  2.7545554493412827  Train-Accuracy:  0.6207458920548957  Val Loss:  7.199486005873907  Val-Accuracy:  0.44944041808410984\n",
            "Epoch:  133  Loss:  2.7136631373203164  Train-Accuracy:  0.6263462366914887  Val Loss:  7.255399635859898  Val-Accuracy:  0.450278558398659\n",
            "Epoch   134: reducing learning rate of group 0 to 7.1790e-05.\n",
            "Epoch:  134  Loss:  2.756657296961004  Train-Accuracy:  0.6201612406917348  Val Loss:  7.24934518904913  Val-Accuracy:  0.4505250702558793\n",
            "Epoch:  135  Loss:  2.7260159290198125  Train-Accuracy:  0.6237614622438304  Val Loss:  7.216083685557048  Val-Accuracy:  0.44919390622688954\n",
            "Epoch:  136  Loss:  2.753658793189309  Train-Accuracy:  0.6208382054280264  Val Loss:  7.243859472728911  Val-Accuracy:  0.4496869299413302\n",
            "Epoch:  137  Loss:  2.740055293747873  Train-Accuracy:  0.6233614376269309  Val Loss:  7.267570064181373  Val-Accuracy:  0.45017995365577085\n",
            "Epoch   138: reducing learning rate of group 0 to 6.4611e-05.\n",
            "Epoch:  138  Loss:  2.708930658571648  Train-Accuracy:  0.6228998707612776  Val Loss:  7.19936770484561  Val-Accuracy:  0.44914460385544547\n",
            "Epoch:  139  Loss:  2.7190249688697583  Train-Accuracy:  0.6263770078158656  Val Loss:  7.236746084122431  Val-Accuracy:  0.44801064931223195\n",
            "Epoch:  140  Loss:  2.7498893954537134  Train-Accuracy:  0.6227460151393932  Val Loss:  7.280591873895554  Val-Accuracy:  0.44914460385544547\n",
            "Epoch:  141  Loss:  2.754822622645985  Train-Accuracy:  0.6224998461443781  Val Loss:  7.214109193711054  Val-Accuracy:  0.4500813489128827\n",
            "Epoch   142: reducing learning rate of group 0 to 5.8150e-05.\n",
            "Epoch:  142  Loss:  2.7237838181582363  Train-Accuracy:  0.622807557388147  Val Loss:  7.247796035948253  Val-Accuracy:  0.4492925109697776\n",
            "Epoch:  143  Loss:  2.7261453397346265  Train-Accuracy:  0.627053972552157  Val Loss:  7.224816549391973  Val-Accuracy:  0.4500813489128827\n",
            "Epoch:  144  Loss:  2.7204070235743667  Train-Accuracy:  0.6236691488706997  Val Loss:  7.2431869733901255  Val-Accuracy:  0.4500813489128827\n",
            "Epoch:  145  Loss:  2.6999970999631016  Train-Accuracy:  0.625730814203951  Val Loss:  7.2365168162754605  Val-Accuracy:  0.4490953014840014\n",
            "Epoch   146: reducing learning rate of group 0 to 5.2335e-05.\n",
            "Epoch:  146  Loss:  2.7279060537164863  Train-Accuracy:  0.6261308388208505  Val Loss:  7.208070777711415  Val-Accuracy:  0.4500813489128827\n",
            "Epoch:  147  Loss:  2.7046385606129966  Train-Accuracy:  0.6277001661640716  Val Loss:  7.242661544254848  Val-Accuracy:  0.450278558398659\n",
            "Epoch:  148  Loss:  2.7091825152888442  Train-Accuracy:  0.62653086343775  Val Loss:  7.203174522944859  Val-Accuracy:  0.44939111571266577\n",
            "Epoch:  149  Loss:  2.6845252802877715  Train-Accuracy:  0.6288386977660164  Val Loss:  7.270968755086263  Val-Accuracy:  0.45003204654143864\n",
            "Epoch   150: reducing learning rate of group 0 to 4.7101e-05.\n",
            "Epoch:  150  Loss:  2.7204654866998847  Train-Accuracy:  0.6244691981044987  Val Loss:  7.281371707007999  Val-Accuracy:  0.45003204654143864\n",
            "Epoch:  151  Loss:  2.707511302196618  Train-Accuracy:  0.628130961905348  Val Loss:  7.206292538415818  Val-Accuracy:  0.45062367499876743\n",
            "Epoch:  152  Loss:  2.7088825196930855  Train-Accuracy:  0.6243461136069912  Val Loss:  7.235722587222145  Val-Accuracy:  0.4507222797416556\n",
            "Epoch:  153  Loss:  2.684676892829664  Train-Accuracy:  0.6305003384823682  Val Loss:  7.259994847433908  Val-Accuracy:  0.44978553468421834\n",
            "Epoch   154: reducing learning rate of group 0 to 4.2391e-05.\n",
            "Epoch:  154  Loss:  2.677978927438909  Train-Accuracy:  0.630254169487353  Val Loss:  7.234856741768973  Val-Accuracy:  0.4494897204555539\n",
            "Epoch:  155  Loss:  2.6878022063862193  Train-Accuracy:  0.6315773278355591  Val Loss:  7.251728216807048  Val-Accuracy:  0.45017995365577085\n",
            "Epoch:  156  Loss:  2.6953914815729316  Train-Accuracy:  0.6236691488706997  Val Loss:  7.310638132549467  Val-Accuracy:  0.4494897204555539\n",
            "Epoch:  157  Loss:  2.7150404525525644  Train-Accuracy:  0.6264693211889962  Val Loss:  7.256917908078148  Val-Accuracy:  0.44914460385544547\n",
            "Epoch   158: reducing learning rate of group 0 to 3.8152e-05.\n",
            "Epoch:  158  Loss:  2.6683938864505654  Train-Accuracy:  0.6299464582435842  Val Loss:  7.249965962909517  Val-Accuracy:  0.4496869299413302\n",
            "Epoch:  159  Loss:  2.6542276469143955  Train-Accuracy:  0.6301003138654686  Val Loss:  7.224365188961937  Val-Accuracy:  0.44983483705566235\n",
            "Epoch:  160  Loss:  2.7075086506930264  Train-Accuracy:  0.6228690996369007  Val Loss:  7.236233052753267  Val-Accuracy:  0.44914460385544547\n",
            "Epoch:  161  Loss:  2.6982117132707075  Train-Accuracy:  0.6282232752784787  Val Loss:  7.230839910961333  Val-Accuracy:  0.44870088251244883\n",
            "Epoch   162: reducing learning rate of group 0 to 3.4337e-05.\n",
            "Epoch:  162  Loss:  2.6822003523508706  Train-Accuracy:  0.6290540956366546  Val Loss:  7.24426928020659  Val-Accuracy:  0.45057437262732336\n",
            "Epoch:  163  Loss:  2.6779020482843574  Train-Accuracy:  0.6309926764723983  Val Loss:  7.254021780831473  Val-Accuracy:  0.45042646551299115\n",
            "Epoch:  164  Loss:  2.6738380302082407  Train-Accuracy:  0.6287463843928858  Val Loss:  7.2431914465768  Val-Accuracy:  0.45062367499876743\n",
            "Epoch:  165  Loss:  2.6655857490770742  Train-Accuracy:  0.6325004615668657  Val Loss:  7.240832056318011  Val-Accuracy:  0.4511166987132081\n",
            "Epoch   166: reducing learning rate of group 0 to 3.0903e-05.\n",
            "Epoch:  166  Loss:  2.6634579716306743  Train-Accuracy:  0.630561880731122  Val Loss:  7.220293817066011  Val-Accuracy:  0.45096879159887593\n",
            "Epoch:  167  Loss:  2.675890944220803  Train-Accuracy:  0.6285617576466244  Val Loss:  7.244529928479876  Val-Accuracy:  0.45032786077010306\n",
            "Epoch:  168  Loss:  2.654873277201797  Train-Accuracy:  0.6278232506615792  Val Loss:  7.260820729391916  Val-Accuracy:  0.45101809397032\n",
            "Epoch:  169  Loss:  2.68361488978068  Train-Accuracy:  0.6279771062834636  Val Loss:  7.254324844905308  Val-Accuracy:  0.45057437262732336\n",
            "Epoch   170: reducing learning rate of group 0 to 2.7813e-05.\n",
            "Epoch:  170  Loss:  2.6409823750004624  Train-Accuracy:  0.6300695427410917  Val Loss:  7.265651089804513  Val-Accuracy:  0.45017995365577085\n",
            "Epoch:  171  Loss:  2.659220550999497  Train-Accuracy:  0.6304695673579913  Val Loss:  7.217936356862386  Val-Accuracy:  0.4505250702558793\n",
            "Epoch:  172  Loss:  2.6671156883239746  Train-Accuracy:  0.630561880731122  Val Loss:  7.319257849738712  Val-Accuracy:  0.4494897204555539\n",
            "Epoch:  173  Loss:  2.6775673591729365  Train-Accuracy:  0.627084743676534  Val Loss:  7.227348759060814  Val-Accuracy:  0.4504757678844352\n",
            "Epoch   174: reducing learning rate of group 0 to 2.5032e-05.\n",
            "Epoch:  174  Loss:  2.6504633715658477  Train-Accuracy:  0.630008000492338  Val Loss:  7.272059962863014  Val-Accuracy:  0.4505250702558793\n",
            "Epoch:  175  Loss:  2.655958385178537  Train-Accuracy:  0.631331158840544  Val Loss:  7.26612461180914  Val-Accuracy:  0.45017995365577085\n",
            "Epoch:  176  Loss:  2.673956553141276  Train-Accuracy:  0.6277309372884485  Val Loss:  7.262439069293794  Val-Accuracy:  0.45003204654143864\n",
            "Epoch:  177  Loss:  2.652062567797574  Train-Accuracy:  0.631608098959936  Val Loss:  7.284212793622698  Val-Accuracy:  0.4500813489128827\n",
            "Epoch   178: reducing learning rate of group 0 to 2.2528e-05.\n",
            "Epoch:  178  Loss:  2.650659098769679  Train-Accuracy:  0.6318850390793279  Val Loss:  7.28175181434268  Val-Accuracy:  0.44939111571266577\n",
            "Epoch:  179  Loss:  2.6599238280094033  Train-Accuracy:  0.6334851375469259  Val Loss:  7.285105977739606  Val-Accuracy:  0.44958832519844205\n",
            "Epoch:  180  Loss:  2.660271081057462  Train-Accuracy:  0.6325004615668657  Val Loss:  7.2982105527605325  Val-Accuracy:  0.45017995365577085\n",
            "Epoch:  181  Loss:  2.6573393128134986  Train-Accuracy:  0.6321927503230969  Val Loss:  7.248089517865862  Val-Accuracy:  0.45017995365577085\n",
            "Epoch   182: reducing learning rate of group 0 to 2.0276e-05.\n",
            "Epoch:  182  Loss:  2.6605967030380713  Train-Accuracy:  0.6324696904424888  Val Loss:  7.239319256373814  Val-Accuracy:  0.4496869299413302\n",
            "Epoch:  183  Loss:  2.6634115667054146  Train-Accuracy:  0.6292387223829159  Val Loss:  7.2716158004034135  Val-Accuracy:  0.45037716314154713\n",
            "Epoch:  184  Loss:  2.6562927491737134  Train-Accuracy:  0.6347775247707551  Val Loss:  7.232735429491315  Val-Accuracy:  0.45037716314154713\n",
            "Epoch:  185  Loss:  2.6624368971044365  Train-Accuracy:  0.6320696658255893  Val Loss:  7.265029339563279  Val-Accuracy:  0.4502292560272149\n",
            "Epoch   186: reducing learning rate of group 0 to 1.8248e-05.\n",
            "Epoch:  186  Loss:  2.67687996228536  Train-Accuracy:  0.6302849406117299  Val Loss:  7.263025011335101  Val-Accuracy:  0.44958832519844205\n",
            "Epoch:  187  Loss:  2.668482440890688  Train-Accuracy:  0.6304695673579913  Val Loss:  7.238002073197138  Val-Accuracy:  0.4501306512843268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d4bc34e6f353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0mbatch_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/VQA-master/models/fusion_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, questions, q_indices)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0mqout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0;31m# print(qout.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0;31m# print(question_embeddings.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/VQA-master/models/fusion_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# return self.gru(v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# print(self.embedding(inp).size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 716\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bB9-AAnSH5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c322d08-0ae0-4854-a9dd-eedc489a68fc"
      },
      "source": [
        "confusion_matrix = np.zeros(shape=[104, 104])\n",
        "class_cnt = np.zeros(shape=[104,1])\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # for i, data in enumerate(trainloader, 0):\n",
        "  for i, data in enumerate(valloader, 0):\n",
        "      # if i % print_every == 0 and i>0:\n",
        "        # print(\"Val batch \",i+1)\n",
        "\n",
        "      if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "        image_features, questions, labels, q_indices = data\n",
        "        # questions = questions/3100\n",
        "        # questions = questions[:,:5]\n",
        "        image_features = image_features.type(torch.float32)\n",
        "        labels = labels.flatten().type(torch.long)\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        outputs = model(image_features, questions, q_indices)\n",
        "\n",
        "      # print(outputs.size())\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().numpy()\n",
        "      labels = labels.detach().numpy()\n",
        "      print(labels.shape)\n",
        "      print(outputs.shape)\n",
        "      for j in range(labels.shape[0]):\n",
        "        class_cnt[labels[i]] += 1\n",
        "        confusion_matrix[labels[i], outputs[i]] += 1\n",
        "      # batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "      # val_correct += batch_correct\n",
        "      # val_total += batch_total\n",
        "      # val_batches += 1\n",
        "\n",
        "      # val_loss = focal_loss(outputs, labels, alpha=alpha, gamma=gamma)\n",
        "      # # val_loss = criterion(outputs, labels)\n",
        "      # val_total_loss += val_loss.detach().numpy()\n",
        "\n",
        "class_cnt += 1e-5\n",
        "confusion_matrix = (confusion_matrix/class_cnt)*100\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(confusion_matrix)\n",
        "plt.show()\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(283,)\n",
            "(283,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1kCCwxTTw9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fce38314-1404-4075-ada4-be540ccbd2c2"
      },
      "source": [
        "accuracy_list = []\n",
        "for i in range(104):\n",
        "  accuracy_list.append((confusion_matrix[i,i], i))\n",
        "\n",
        "accuracy_list.sort()\n",
        "print(accuracy_list[:10])\n",
        "accuracy_list.reverse()\n",
        "print(accuracy_list[:10])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0.0, 0), (0.0, 2), (0.0, 3), (0.0, 4), (0.0, 5), (0.0, 6), (0.0, 7), (0.0, 8), (0.0, 9), (0.0, 10)]\n",
            "[(99.99999900000002, 97), (99.99999900000002, 88), (99.99999900000002, 79), (99.99999900000002, 69), (99.99999900000002, 53), (99.99999900000002, 51), (99.99999900000002, 11), (99.99999900000002, 1), (49.99999975, 64), (0.0, 103)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNwQjJEbUKWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "702edeb5-52e5-4809-9dd5-aecec3e71ffa"
      },
      "source": [
        "correct_arr = np.zeros(shape=[104, 1])\n",
        "total_arr = np.zeros(shape=[104, 1])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # for i, data in enumerate(trainloader, 0):\n",
        "  for i, data in enumerate(valloader, 0):\n",
        "      # if i % print_every == 0 and i>0:\n",
        "        # print(\"Val batch \",i+1)\n",
        "\n",
        "      if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "        image_features, questions, labels, q_indices = data\n",
        "        # questions = questions/3100\n",
        "        # questions = questions[:,:5]\n",
        "        image_features = image_features.type(torch.float32)\n",
        "        labels = labels.flatten().type(torch.long)\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        outputs = model(image_features, questions, q_indices)\n",
        "\n",
        "      # print(outputs.size())\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().numpy()\n",
        "      labels = labels.detach().numpy()\n",
        "      print(labels.shape)\n",
        "      print(outputs.shape)\n",
        "      for j in range(labels.shape[0]):\n",
        "        if labels[j] == outputs[j]:\n",
        "          correct_arr[labels[j]] += 1\n",
        "        total_arr[labels[j]] += 1\n",
        "        \n",
        "accuracy_list = []\n",
        "\n",
        "for i in range(104):\n",
        "  v = (correct_arr[i,0]/(total_arr[i,0]+1e-8))*100.0\n",
        "  accuracy_list.append((v, answer_dict[i], correct_arr[i,0], total_arr[i,0]))\n",
        "\n",
        "accuracy_list.sort()\n",
        "print(accuracy_list[:10])\n",
        "accuracy_list.reverse()\n",
        "print(accuracy_list[:10])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(283,)\n",
            "(283,)\n",
            "[(0.6289308175705075, '15', 1.0, 159.0), (1.4925373133585802, '7', 3.0, 201.0), (1.4925373133585802, '9', 3.0, 201.0), (2.487562188930967, '12', 5.0, 201.0), (2.487562188930967, 'purple', 5.0, 201.0), (3.4825870645033534, '6', 7.0, 201.0), (4.102564102353715, 'many', 8.0, 195.0), (4.47761194007574, '10', 9.0, 201.0), (4.47761194007574, '20', 9.0, 201.0), (4.975124377861934, '8', 10.0, 201.0)]\n",
            "[(88.0434782560846, 'night', 162.0, 184.0), (83.0845771102943, 'wii', 167.0, 201.0), (82.5870646725081, 'baseball', 166.0, 201.0), (81.09452735914952, 'giraffe', 163.0, 201.0), (79.60199004579094, 'broccoli', 160.0, 201.0), (79.10447760800476, 'stop', 159.0, 201.0), (78.60696517021856, 'bathroom', 158.0, 201.0), (77.61194029464616, 'tennis', 156.0, 201.0), (77.61194029464616, 'surfing', 156.0, 201.0), (77.11442785685998, 'skiing', 155.0, 201.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_f8MQsFYVBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "18a6170f-4a45-4b0f-b1c9-450b2d51167a"
      },
      "source": [
        "x = np.arange(1, 11)*1\n",
        "plt.figure(figsize=(16,8))\n",
        "accuracy_list.sort()\n",
        "y = np.zeros_like(x, dtype=np.float32)\n",
        "c = ['red', 'blue', 'green', 'gray', 'orange']\n",
        "labels = []\n",
        "for i in range(10):\n",
        "  y[i] = accuracy_list[i][0]\n",
        "  labels.append(accuracy_list[i][1])\n",
        "plt.bar(x, y, color=c, tick_label=labels, width=0.5)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.ylabel(\"Accuracy %\", fontdict={'size': 20})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps7OHYCfbHbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "ebb9ce36-ee73-4aaa-dde3-1ea3a343e671"
      },
      "source": [
        "x = np.arange(1, 11)*1\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "accuracy_list.sort()\n",
        "accuracy_list.reverse()\n",
        "y = np.zeros_like(x, dtype=np.float32)\n",
        "c = ['red', 'blue', 'green', 'gray', 'orange']\n",
        "labels = []\n",
        "for i in range(10):\n",
        "  y[i] = accuracy_list[i][0]\n",
        "  labels.append(accuracy_list[i][1])\n",
        "plt.bar(x, y, color=c, tick_label=labels, width=0.5)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.ylabel(\"Accuracy %\", fontdict={'size': 20})\n",
        "plt.show()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnI-1ImKr-lU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b75821df-c17c-45c6-a41d-8c1aec45b4bc"
      },
      "source": [
        "print(answer_dict)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'no', 1: 'yes', 2: '2', 3: '1', 4: 'white', 5: '3', 6: 'black', 7: 'red', 8: 'blue', 9: '0', 10: '4', 11: 'brown', 12: 'green', 13: 'yellow', 14: '5', 15: 'gray', 16: 'nothing', 17: 'right', 18: 'left', 19: 'frisbee', 20: 'baseball', 21: '6', 22: 'wood', 23: 'tennis', 24: 'orange', 25: 'none', 26: 'pizza', 27: 'kitchen', 28: 'bathroom', 29: 'cat', 30: 'pink', 31: '7', 32: '8', 33: 'skiing', 34: 'skateboarding', 35: 'man', 36: 'grass', 37: 'water', 38: 'dog', 39: 'silver', 40: 'surfing', 41: '10', 42: 'black and white', 43: 'kite', 44: 'horse', 45: 'skateboard', 46: 'phone', 47: 'wii', 48: 'living room', 49: 'stop', 50: 'giraffe', 51: 'eating', 52: 'woman', 53: 'surfboard', 54: 'broccoli', 55: 'sunny', 56: 'hat', 57: 'cake', 58: 'snow', 59: '9', 60: 'standing', 61: 'banana', 62: 'purple', 63: 'food', 64: 'table', 65: 'camera', 66: 'beach', 67: 'male', 68: 'elephant', 69: 'train', 70: 'flowers', 71: 'snowboarding', 72: '12', 73: 'sheep', 74: 'laptop', 75: 'apple', 76: 'cow', 77: 'winter', 78: 'trees', 79: 'wine', 80: 'walking', 81: 'female', 82: 'clear', 83: 'tan', 84: 'metal', 85: 'soccer', 86: 'brick', 87: 'bird', 88: '20', 89: 'tile', 90: 'outside', 91: 'bus', 92: 'donut', 93: 'motorcycle', 94: 'umbrella', 95: 'summer', 96: 'car', 97: 'night', 98: 'plane', 99: 'kites', 100: 'bear', 101: '15', 102: 'cloudy', 103: 'many'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaVg83dEsB_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "fc67f38d-e4c0-49ee-a1ba-2c33530e6097"
      },
      "source": [
        "# correct_arr = np.zeros(shape=[104, 1])\n",
        "# total_arr = np.zeros(shape=[104, 1])\n",
        "pred_arr = np.zeros(shape=[104, 1])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # for i, data in enumerate(trainloader, 0):\n",
        "  for i, data in enumerate(valloader, 0):\n",
        "      # if i % print_every == 0 and i>0:\n",
        "        # print(\"Val batch \",i+1)\n",
        "\n",
        "      if model_input == \"resnet 18 image features, question\" or model_input == \"resnet 192 image features, question\" or model_input == \"new resnet 18 features\" or model_input == \"new resnet 152 features\":\n",
        "        image_features, questions, labels, q_indices = data\n",
        "        # questions = questions/3100\n",
        "        # questions = questions[:,:5]\n",
        "        image_features = image_features.type(torch.float32)\n",
        "        labels = labels.flatten().type(torch.long)\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        outputs = model(image_features, questions, q_indices)\n",
        "\n",
        "      # print(outputs.size())\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().numpy()\n",
        "      labels = labels.detach().numpy()\n",
        "      print(labels.shape)\n",
        "      print(outputs.shape)\n",
        "      for j in range(labels.shape[0]):\n",
        "        # if labels[j] == outputs[j]:\n",
        "          # correct_arr[labels[j]] += 1\n",
        "        # total_arr[labels[j]] += 1\n",
        "        pred_arr[outputs[j]] += 1\n",
        "        \n",
        "# accuracy_list = []\n",
        "\n",
        "# for i in range(104):\n",
        "#   v = (correct_arr[i,0]/(total_arr[i,0]+1e-8))*100.0\n",
        "#   accuracy_list.append((v, answer_dict[i], correct_arr[i,0], total_arr[i,0]))\n",
        "\n",
        "# accuracy_list.sort()\n",
        "# print(accuracy_list[:10])\n",
        "# accuracy_list.reverse()\n",
        "# print(accuracy_list[:10])\n",
        "pred_list = []\n",
        "\n",
        "for i in range(104):\n",
        "  v = pred_arr[i,0]\n",
        "  pred_list.append((v, answer_dict[i]))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(1000,)\n",
            "(283,)\n",
            "(283,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvWM3GlupRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "48de9f0d-3cd0-4286-a915-a86e5a660ff3"
      },
      "source": [
        "pred_list.sort()\n",
        "pred_list.reverse()\n",
        "print(pred_list)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7e8b99c69b08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJjmCeA2uuA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = 100\n",
        "x = np.arange(1, num+1)*1\n",
        "\n",
        "fig, axes = plt.subplots(1, 1, figsize=(320, 160), dpi=200)\n",
        "# plt.figure(figsize=(320,160))\n",
        "pred_list.sort()\n",
        "pred_list.reverse()\n",
        "y = np.zeros_like(x, dtype=np.float32)\n",
        "c = ['red', 'blue', 'green', 'gray', 'orange']\n",
        "labels = []\n",
        "for i in range(num):\n",
        "  y[i] = pred_list[i][0]\n",
        "  labels.append(pred_list[i][1])\n",
        "plt.bar(x, y, color=c, tick_label=labels, width=0.5)\n",
        "plt.xticks(fontsize=200)\n",
        "plt.yticks(fontsize=200)\n",
        "plt.ylabel(\"Predictions Frequency\", fontdict={'size': 20})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPSIkY91vfjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXJUKpI2XJiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "  \n",
        "# JSON file\n",
        "def read_json(filename): \n",
        "  f = open (filename, \"r\") \n",
        "    \n",
        "  # Reading from file \n",
        "  data = json.loads(f.read()) \n",
        "  return data\n",
        "\n",
        "dataset = read_json(\"PythonHelperTools/combined_filtered_train_dataset_32k.json\")\n",
        "\n",
        "answer_dict = {}\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  # print(dataset[i])\n",
        "  # break\n",
        "  ans = dataset[i]['answer']\n",
        "  ans_id = dataset[i]['answer_id']\n",
        "  if ans_id not in answer_dict:\n",
        "    answer_dict[ans_id] = ans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCd9IxVU3yD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcnt_dict = {}\n",
        "\n",
        "for i in range(len(train_answers)):\n",
        "  v = train_answers[i][0]\n",
        "  if v not in cnt_dict:\n",
        "    cnt_dict[v] = 1\n",
        "  else:\n",
        "    cnt_dict[v] = cnt_dict[v] + 1\n",
        "print(cnt_dict)\n",
        "\n",
        "cnt_list = []\n",
        "for key in cnt_dict.keys():\n",
        "  v = cnt_dict[key]\n",
        "  if v > 50:\n",
        "    cnt_list.append((key,v))\n",
        "\n",
        "print(len(cnt_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jXAs9MaAAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_questions.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFWIgHK2NISi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_questions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2RbyPLtNKLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "faa9aba4-8fde-4923-87ef-beea09c03155"
      },
      "source": [
        "print(train_questions[100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.970e+02 1.090e+02 6.900e+01 3.200e+01 5.015e+03 4.000e+05 4.000e+05\n",
            " 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05\n",
            " 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05 4.000e+05\n",
            " 4.000e+05 4.000e+05 4.000e+05 4.000e+05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTg1-HhjNMB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ti = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1TuNN2IYluE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = torch.tensor([[[1,11],[2,12]],[[3,13],[4,14]]])\n",
        "print(t.size())\n",
        "r = torch.gather(t, 1, torch.tensor([[[0,0]],[[1,1]]]))\n",
        "print(r.size())\n",
        "print(t)\n",
        "print(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_iCwjGrYnYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zppoDP8EITS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "answer_type = \"number\"\n",
        "\n",
        "\n",
        "train_image = np.load(\"vgg_train_image_features_\"+answer_type+\".npy\")\n",
        "train_image = np.reshape(train_image, newshape=[train_image.shape[0], train_image.shape[1], train_image.shape[2]*train_image.shape[3]])\n",
        "print(train_image.shape)\n",
        "\n",
        "val_image = np.load(\"vgg_val_image_features_\"+answer_type+\".npy\")\n",
        "val_image = np.reshape(val_image, newshape=[val_image.shape[0], val_image.shape[1], val_image.shape[2]*val_image.shape[3]])\n",
        "print(val_image.shape)\n",
        "\n",
        "train_question = np.load(\"train_questions_\"+answer_type+\".npy\")\n",
        "val_question = np.load(\"val_questions_\"+answer_type+\".npy\")\n",
        "\n",
        "qword_dict = {}\n",
        "cnt = 0\n",
        "for i in range(train_question.shape[0]):\n",
        "  for j in range(train_question.shape[1]):\n",
        "    word = train_question[i,j]\n",
        "    if word not in qword_dict:\n",
        "      qword_dict[word] = cnt\n",
        "      cnt += 1\n",
        "\n",
        "for i in range(val_question.shape[0]):\n",
        "  for j in range(val_question.shape[1]):\n",
        "    word = val_question[i,j]\n",
        "    if word not in qword_dict:\n",
        "      qword_dict[word] = cnt\n",
        "      cnt += 1\n",
        "\n",
        "for i in range(train_question.shape[0]):\n",
        "  for j in range(train_question.shape[1]):\n",
        "    word = train_question[i,j]\n",
        "    train_question[i,j] = qword_dict[word]\n",
        "\n",
        "for i in range(val_question.shape[0]):\n",
        "  for j in range(val_question.shape[1]):\n",
        "    word = val_question[i,j]\n",
        "    val_question[i,j] = qword_dict[word]\n",
        "\n",
        "# print(\"cnt = \",cnt)\n",
        "\n",
        "train_answer = np.load(\"train_answers_\"+answer_type+\".npy\").astype(int)\n",
        "# print(train_question.shape)\n",
        "# print(train_answer.shape)\n",
        "# print(train_answer.dtype)\n",
        "\n",
        "val_answer = np.load(\"val_answers_\"+answer_type+\".npy\").astype(int)\n",
        "# print(val_question.shape)\n",
        "# print(val_answer.shape)\n",
        "# print(val_answer.dtype)\n",
        "\n",
        "# print(train_question[:2])\n",
        "# print(val_question[:2])\n",
        "\n",
        "#all_images = np.concatenate((train_image,val_image), axis=0)\n",
        "#all_questions = np.concatenate((train_question,val_question),axis=0)\n",
        "#all_answers = np.concatenate((train_answer,val_answer), axis = 0)\n",
        "\n",
        "#print(all_images.shape)\n",
        "#print(all_questions.shape)\n",
        "#print(all_answers.shape)\n",
        "\n",
        "#indices = np.arange(6000)\n",
        "#random.shuffle(indices)\n",
        "\n",
        "#all_images = all_images[indices,:]\n",
        "#all_questions = all_questions[indices,:]\n",
        "#all_answers = all_answers[indices, :]\n",
        "\n",
        "#train_image = all_images [:4000, :]\n",
        "#val_image = all_images[4000:,:]\n",
        "\n",
        "#train_question = all_questions[:4000,:]\n",
        "#val_question = all_questions[4000:,:]\n",
        "\n",
        "#train_answer = all_answers[:4000,:].astype(int)\n",
        "#val_answer = all_answers[4000:,:].astype(int)\n",
        "\n",
        "#print(train_image.shape)\n",
        "#print(train_question.shape)\n",
        "#print(train_answer.shape)\n",
        "\n",
        "#print(val_image.shape)\n",
        "#print(val_question.shape)\n",
        "#print(val_answer.shape)\n",
        "\n",
        "#a ,b = np.unique(train_answer, return_counts=True)\n",
        "#print(a)\n",
        "#print(b)\n",
        "\n",
        "#a ,b = np.unique(val_answer, return_counts=True)\n",
        "#print(a)\n",
        "#print(b)\n",
        "\n",
        "class ParallelCoAttention(nn.Module):\n",
        "  def __init__(self, d, t, k, vocab_size, dropout):\n",
        "    super(ParallelCoAttention, self).__init__()\n",
        "    self.d = d \n",
        "    self.embedding_dim = d\n",
        "    self.t = t\n",
        "    self.k = k\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.W_b = nn.Linear(self.d, self.d)\n",
        "    self.W_q = nn.Linear(self.d, self.k)\n",
        "    self.W_v = nn.Linear(self.d, self.k)\n",
        "    self.w_hv = nn.Linear(self.k, 1)\n",
        "    self.w_hq = nn.Linear(self.k, 1)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim =1)\n",
        "\n",
        "\n",
        "  def forward(self, questions, images):\n",
        "    # print(\"here 1\")\n",
        "    questions = self.embedding(questions)\n",
        "    # print(\"here 2\")\n",
        "    x = self.dropout(self.W_b(questions))#self.W_b(questions)\n",
        "    # print(\"here 3\")\n",
        "    C = self.tanh(torch.bmm(x,images)) \n",
        "    # print(\"here 4\")\n",
        "\n",
        "    H_v = self.tanh(self.dropout(self.W_v(torch.transpose(images, 1, 2)) + torch.bmm(torch.transpose(C,1,2),self.W_q(questions)))) #self.tanh(self.W_v(torch.transpose(images, 1, 2)) + torch.bmm(torch.transpose(C,1,2),self.W_q(questions)))\n",
        "    # print(\"here 5\")\n",
        "    a_v = self.softmax(self.dropout(self.w_hv(H_v)))#self.softmax(self.w_hv(H_v))\n",
        "    # print(\"here 6\")\n",
        "    a_v = torch.transpose(a_v, 1,2) \n",
        "    # print(\"here 7\")\n",
        "    v_hat = torch.sum(a_v * images, axis= 2) \n",
        "    # print(\"here 8\")\n",
        "\n",
        "    H_q = self.tanh(self.dropout(self.W_q(questions) + torch.bmm(C,self.W_v(torch.transpose(images, 1, 2))))) #self.tanh(self.W_q(questions) + torch.bmm(C,self.W_v(torch.transpose(images, 1, 2))))\n",
        "    # print(\"here 9\")\n",
        "    a_q = self.softmax(self.dropout(self.w_hq(H_q)))#self.softmax(self.w_hq(H_q))\n",
        "    # print(\"here 10\")\n",
        "    q_hat = torch.sum(a_q*questions, axis =1) \n",
        "    # print(\"here 11\")\n",
        "    return (q_hat, v_hat)\n",
        "\n",
        "class AlternateCoAttention(nn.Module):\n",
        "  def __init__(self, d, k, vocab_size, dropout):\n",
        "    super(AlternateCoAttention, self).__init__()\n",
        "    self.d = d\n",
        "    self.k = k\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.d)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.W_x_1 = nn.Linear(self.d, self.k)\n",
        "    self.W_g_1 = nn.Linear(self.d, self.k)\n",
        "    self.W_hx_1 = nn.Linear(self.k, 1)\n",
        "\n",
        "    self.W_x_2 = nn.Linear(self.d, self.k)\n",
        "    self.W_g_2 = nn.Linear(self.d, self.k)\n",
        "    self.W_hx_2 = nn.Linear(self.k, 1)\n",
        "\n",
        "    self.W_x_3 = nn.Linear(self.d, self.k)\n",
        "    self.W_g_3 = nn.Linear(self.d, self.k)\n",
        "    self.W_hx_3 = nn.Linear(self.k, 1)\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, questions, images):\n",
        "    # print(\"there 1\")\n",
        "    questions = self.embedding(questions)\n",
        "    # print(\"there 2\")\n",
        "    H_1 = self.tanh(self.dropout(self.W_x_1(questions)))#self.tanh(self.W_x_1(questions))\n",
        "    # print(\"there 3\")\n",
        "    a_x_1 = self.softmax(self.dropout(self.W_hx_1(H_1))) #self.softmax(self.W_hx_1(H_1))\n",
        "    # print(\"there 4\")\n",
        "    x_hat_1 = torch.sum(a_x_1 * questions, axis=1) \n",
        "\n",
        "    # print(\"there 5\")\n",
        "    H_2 = self.tanh(self.dropout(self.W_x_2(torch.transpose(images,1,2)) + self.W_g_2(x_hat_1).unsqueeze(1)))#self.tanh(self.W_x_2(torch.transpose(images,1,2)) + self.W_g_2(x_hat_1).unsqueeze(1))\n",
        "    # print(\"there 6\")\n",
        "    a_x_2 = self.softmax(self.dropout(self.W_hx_2(H_2)))#self.softmax(self.W_hx_2(H_2))\n",
        "    # print(\"there 7\")\n",
        "    x_hat_2 = torch.sum(a_x_2*torch.transpose(images,1,2), axis=1)\n",
        "\n",
        "    # print(\"there 8\")\n",
        "    H_3 = self.tanh(self.dropout(self.W_x_3(questions) + self.W_g_3(x_hat_2).unsqueeze(1)))#self.tanh(self.W_x_3(questions) + self.W_g_3(x_hat_2).unsqueeze(1))\n",
        "    # print(\"there 9\")\n",
        "    a_x_3 = self.softmax(self.dropout(self.W_hx_3(H_3)))#self.softmax(self.W_hx_3(H_3))\n",
        "    # print(\"there 10\")\n",
        "    x_hat_3 = torch.sum(a_x_3*questions, axis=1)\n",
        "    # print(\"there 11\")\n",
        "    return (x_hat_2, x_hat_3)\n",
        "\n",
        "class AnswerGeneration(nn.Module):\n",
        "  def __init__(self, d, d_prime, dropout):\n",
        "    super(AnswerGeneration, self).__init__()\n",
        "    self.d = d\n",
        "    self.d_prime = d_prime\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "    self.W = nn.Linear(self.d, self.d_prime)\n",
        "    self.W_h = nn.Linear(self.d_prime,  100) #no of classes for yesno -2, number - 100, other - 1000 --verify again \n",
        "\n",
        "  def forward(self, q_hat, v_hat):\n",
        "    h = self.tanh(self.dropout(self.W(q_hat + v_hat)))#self.tanh(self.W(q_hat + v_hat))\n",
        "    return self.W_h(h)#self.softmax(self.W_h(h))\n",
        "\n",
        "class Fusion(nn.Module):\n",
        "  def __init__(self, shared_size):\n",
        "    self.shared_size = shared_size\n",
        "    super(Fusion, self).__init__()\n",
        "    self.qp_qa = nn.Linear(512, 512)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    self.qpa_qs = nn.Linear(512, self.shared_size)\n",
        "    self.bn2 = nn.BatchNorm1d(self.shared_size)\n",
        "\n",
        "    self.qa_qp = nn.Linear(512, 512)\n",
        "    self.bn3 = nn.BatchNorm1d(512)\n",
        "    self.qap_qs = nn.Linear(512, self.shared_size)\n",
        "    self.bn4 = nn.BatchNorm1d(self.shared_size)\n",
        "\n",
        "    self.vp_va = nn.Linear(512, 512)\n",
        "    self.bn5 = nn.BatchNorm1d(512)\n",
        "    self.vpa_vs = nn.Linear(512, self.shared_size)\n",
        "    self.bn6 = nn.BatchNorm1d(self.shared_size)\n",
        "\n",
        "    self.va_vp = nn.Linear(512, 512)\n",
        "    self.bn7 = nn.BatchNorm1d(512)\n",
        "    self.vap_vs = nn.Linear(512, self.shared_size)\n",
        "    self.bn8 = nn.BatchNorm1d(self.shared_size)\n",
        "\n",
        "  def forward(self, q_hat_p, v_hat_p, q_hat_a, v_hat_a):\n",
        "    fq1 = self.bn2(self.qpa_qs((self.bn1(self.qp_qa(q_hat_p)+q_hat_a))))\n",
        "    fq2 = self.bn4(self.qap_qs((self.bn3(self.qa_qp(q_hat_a)+q_hat_p))))\n",
        "    fq = fq1*fq2\n",
        "\n",
        "    fv1 = self.bn6(self.vpa_vs((self.bn5(self.vp_va(v_hat_p)+v_hat_a))))\n",
        "    fv2 = self.bn8(self.vap_vs((self.bn7(self.va_vp(v_hat_a)+v_hat_p))))\n",
        "    fv = fv1*fv2\n",
        "\n",
        "    return fq, fv\n",
        "\n",
        "\n",
        "class MainModel(nn.Module):\n",
        "  def __init__(self, d, t, k, d_prime, vocab_size, dropout, shared_size):\n",
        "    super(MainModel, self).__init__()\n",
        "    self.d = d\n",
        "    self.t = t\n",
        "    self.k = k \n",
        "    self.d_prime = d_prime\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = dropout\n",
        "    self.shared_size = shared_size\n",
        "\n",
        "    self.parallel = ParallelCoAttention(self.d, self.t, self.k, self.vocab_size, self.dropout)\n",
        "    self.alternate = AlternateCoAttention(self.d, self.k, self.vocab_size, self.dropout)\n",
        "    self.fusion = Fusion(shared_size)\n",
        "    self.answer = AnswerGeneration(self.shared_size, self.d_prime, self.dropout)\n",
        "    \n",
        "\n",
        "\n",
        "  def forward(self, questions, images):\n",
        "    # print(\"parallel start\")\n",
        "    q_hat_p, v_hat_p = self.parallel(questions, images)\n",
        "    # print(\"parallel end\")\n",
        "    # print(\"alt start\")\n",
        "    v_hat_a, q_hat_a = self.alternate(questions, images)\n",
        "    # print(q_hat_p.size())\n",
        "    # print(v_hat_p.size())\n",
        "    # print(q_hat_a.size())\n",
        "    # print(v_hat_a.size())\n",
        "    fq, fv = self.fusion(q_hat_p, v_hat_p, q_hat_a, v_hat_a)\n",
        "    # print(\"alt end\")\n",
        "    # answer_p = self.answer(q_hat_p, v_hat_p)\n",
        "    # answer_a = self.answer(q_hat_a, v_hat_a)\n",
        "    answer = self.answer(fq, fv)\n",
        "    return answer\n",
        "    # return (answer_p, answer_a)\n",
        "\n",
        "d = 512\n",
        "t = 25\n",
        "k = 512\n",
        "d_prime = 128 \n",
        "vocab_size = cnt\n",
        "dropout = 0.5\n",
        "shared_size = 129\n",
        "\n",
        "model = MainModel(d, t, k, d_prime, vocab_size, dropout, shared_size)\n",
        "# model.load_state_dict(torch.load(\"output/model.pt\"))\n",
        "\n",
        "\n",
        "tensor_x = torch.Tensor(train_question).type(torch.long)\n",
        "tensor_y = torch.Tensor(train_image).type(torch.float)\n",
        "tensor_z = torch.Tensor(train_answer).type(torch.long).squeeze()\n",
        "\n",
        "trainset = data.TensorDataset(tensor_x, tensor_y, tensor_z)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 1000, shuffle = True, num_workers=2)\n",
        "\n",
        "tensor_x = torch.Tensor(val_question).type(torch.long)\n",
        "tensor_y = torch.Tensor(val_image).type(torch.float)\n",
        "tensor_z = torch.Tensor(val_answer).type(torch.long).squeeze()\n",
        "\n",
        "valset = data.TensorDataset(tensor_x, tensor_y, tensor_z)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size = 1000, shuffle = True, num_workers = 2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=4e-5, weight_decay=1e-6, momentum=0.99)\n",
        "\n",
        "def get_accuracy(predictions, labels):\n",
        "  predictions = F.softmax(predictions,dim=1)\n",
        "  predictions = torch.max(predictions, axis=1)[1]\n",
        "  #predictions = predictions.detach().numpy()\n",
        "  #correct = predictions.eq(labels).sum()\n",
        "  ab = torch.abs(predictions-labels)\n",
        "  ab = ab.detach().numpy()\n",
        "  mn = np.minimum(ab, 1)\n",
        "  eq = 1-mn\n",
        "  correct = np.sum(eq)\n",
        "  total = eq.shape[0]\n",
        "  #total = predictions.shape[0]\n",
        "  return correct, total\n",
        "\n",
        "train_loss_plot = []\n",
        "val_loss_plot = []\n",
        "\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        questions, images, labels = data\n",
        "      \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # outputs_p, outputs_a = model(questions, images)\n",
        "        # batch_correct, batch_total = get_accuracy(outputs_p, labels)\n",
        "        outputs = model(questions, images)\n",
        "        batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "        correct += batch_correct\n",
        "        total += batch_total\n",
        "        # loss = criterion(outputs_a, labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        total_loss += running_loss\n",
        "        running_loss = 0.0\n",
        "    \n",
        "\n",
        "    running_val_loss = 0.0\n",
        "    total_val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(valloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          questions, images, labels = data\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          # outputs_p, outputs_a = model(questions, images)\n",
        "          # batch_correct, batch_total = get_accuracy(outputs_p, labels)\n",
        "          outputs = model(questions, images)\n",
        "          batch_correct, batch_total = get_accuracy(outputs, labels)\n",
        "          val_correct += batch_correct\n",
        "          val_total += batch_total\n",
        "          # loss = criterion(outputs_a, labels)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          running_val_loss += loss.item()\n",
        "          total_val_loss += running_val_loss\n",
        "          running_val_loss = 0.0\n",
        "          \n",
        "    # with open(\"output/output.txt\",\"a\") as f:\n",
        "    #     f.write(\"Epoch: \"+str(epoch)+\" Train Loss: \"+str(total_loss)+\" Val Loss \"+str(total_val_loss)+\" Train Correct \"+str(correct)+\" Val Correct \"+str(val_correct)+\" Train-Accuracy: \"+str(correct/total)+\" Val-Accuracy: \"+str(val_correct/val_total))\n",
        "    #     f.write(\"\\n\")\n",
        "    # f.close()\n",
        "\n",
        "    print(\"Epoch: \",epoch, \" Train Loss \",total_loss,\" Val Loss \", total_val_loss, \" Train Accuracy \",correct/total, \" Val Accuracy \", val_correct/val_total)\n",
        "\n",
        "    train_loss_plot.append(total_loss)\n",
        "    val_loss_plot.append(total_val_loss)\n",
        "\n",
        "    # with open(\"output/train_loss.txt\",\"a\") as f:\n",
        "    #     f.write(str(total_loss))\n",
        "    #     f.write(\"\\n\")\n",
        "    # f.close()\n",
        "   \n",
        "    # with open(\"output/val_loss.txt\",\"a\") as f:\n",
        "    #     f.write(str(total_val_loss))\n",
        "    #     f.write(\"\\n\")\n",
        "    # f.close()\n",
        "    # torch.save(model.state_dict(),\"output/model.pt\")\n",
        "    #plt.plot(np.arange(epoch+1),train_loss_plot)\n",
        "    #plt.plot(np.arange(epoch+1),val_loss_plot)\n",
        "    #plt.show()\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUtEwZovGGPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}